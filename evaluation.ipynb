{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2f78310-5218-4d40-9ac9-f58cc59a9830",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import re\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\n",
    "from huggingface_hub import HfApi, HfFolder\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3090abe4-ef40-4ab7-9a04-5f02d2ba3286",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "538b08e40a034d6fafd474cce70d44b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Load the model\n",
    "\n",
    "# Load the base model (Mistral 7B)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", quantization_config=bnb_config, device_map=\"auto\")\n",
    "\n",
    "# Load the fine-tuned LoRA adapters from your repository on Hugging Face\n",
    "# peft_model = PeftModel.from_pretrained(base_model, \"Saef/mistral_attention_all_100_epoch\")\n",
    "\n",
    "checkpoint_path = \"checkpoints/mistral_attention_new/checkpoint-24100\" #No explanation\n",
    "# Load PEFT model with the checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path) #Edit here\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\") #edit here\n",
    "peft_model = PeftModel.from_pretrained(base_model, checkpoint_path)\n",
    "                \n",
    "# Load the tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Saef/mistral_attention_all_100_epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "035c6952-0d69-4dd1-8f41-6382e626a74a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# Define custom stopping criterion that halts when \"###\" appears again after \"Target communities\"\n",
    "# class StopOnHashCriteriaAfterTarget(StoppingCriteria):\n",
    "#     def __init__(self, stop_token, after_token):\n",
    "#         self.stop_token = stop_token.lower()\n",
    "#         self.after_token = after_token.lower()\n",
    "\n",
    "#     def __call__(self, input_ids, scores, **kwargs):\n",
    "#         # Decode generated token IDs back to a readable string\n",
    "#         generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True).lower()\n",
    "        \n",
    "#         # Stop if \"###\" appears after \"Target communities\"\n",
    "#         if self.after_token in generated_text:\n",
    "#             return self.stop_token in generated_text.split(self.after_token)[-1]  # Only consider \"###\" after \"Target communities\"\n",
    "#         return False\n",
    "\n",
    "# Create stopping criteria\n",
    "# stopping_criteria = StoppingCriteriaList([StopOnHashCriteriaAfterTarget(\"###\", \"Target Communities:\")])\n",
    "\n",
    "class StopAfterExplanationCriteria(StoppingCriteria):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        for i in range(input_ids.shape[0]):  # Loop through batch\n",
    "            generated_text = self.tokenizer.decode(input_ids[i], skip_special_tokens=True).lower()\n",
    "\n",
    "            explanation_end = generated_text.find(\"### explanation:\")\n",
    "            sentence_start = generated_text.find(\"### sentence:\", explanation_end)\n",
    "\n",
    "            if explanation_end != -1 and sentence_start != -1:\n",
    "                return True  # One example has finished â€” stop the whole batch\n",
    "        return False  # Continue generation\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopAfterExplanationCriteria(tokenizer)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5226d0de-2fd0-4ba1-843c-fff6e141ab60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1922\n",
      "{'id': '1178613994371928065_twitter', 'post_tokens': ['me', 'getting', 'books', 'from', 'the', 'library', 'about', 'queer', 'cowboys', 'and', 'homoeroticism', 'in', 'cinema', 'instead', 'of', 'anything', 'useful', 'for', 'my', 'thesis'], 'label': 1, 'target': ['None'], 'rationale': [], 'explanation': 'None'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MistralForCausalLM(\n",
       "      (model): MistralModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x MistralDecoderLayer(\n",
       "            (self_attn): MistralSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): MistralRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): MistralMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (lm_head): lora.Linear(\n",
       "        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (default): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (default): Linear(in_features=32, out_features=32000, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "        (lora_magnitude_vector): ModuleDict()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "\n",
    "# Load your evaluation data\n",
    "with open(\"dataset/e1_easy_eval_data.json\", \"r\") as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "print(len(eval_data))\n",
    "# Limit to first 100 samples for testing\n",
    "# eval_data = eval_data[:100]\n",
    "eval_data = eval_data[:30]\n",
    "# Convert to Hugging Face Dataset\n",
    "hf_eval_dataset = Dataset.from_list(eval_data)\n",
    "print(hf_eval_dataset[0])\n",
    "\n",
    "def generate_prompt(sentence):\n",
    "    prompt = (f\"Classify whether the sentence is hate speech, offensive, or normal and provide a list of target communities.\\n\\n\"\n",
    "            f\"### Sentence:\\n{sentence}\\n\\n\")\n",
    "    return prompt\n",
    "\n",
    "\n",
    "label_mapping = {0: \"hate speech\", 1: \"normal\", 2: \"offensive\", 3: \"undecided\"}\n",
    "reverse_label_mapping = {v.lower(): k for k, v in label_mapping.items()}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "peft_model.to(device)\n",
    "peft_model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65b2c211-d794-4fe7-acb4-a7c18f77d8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hate speech ['Women', 'Islam'] this sentence is hate speech because it uses discriminatory, prejorative, and violent language against Women and Islam through words 'muzzie shithole'.\n",
      "Classify whether the sentence is hate speech, offensive, or normal and provide a list of target communities.\n",
      "\n",
      "### Sentence:\n",
      "democrats from minnesota leftist muzzie shithole are the dumbest anti americans there are klobuchar one of the reps is a dumb bitch who has a moronic position on the tax bill fuck this stupid woman\n",
      "\n",
      "### Classification:\n",
      " hate speech \n",
      "\n",
      "### Target Communities:\n",
      " Islam, Women \n",
      "\n",
      "### Explanation:\n",
      " this sentence is hate speech because it uses discriminatory, prejorative, and violent language against Islam and Women through words 'muzzie shithole' and 'dumb bitch'. \n",
      "\n",
      "\n",
      "offensive [] this sentence is offensive because it contains insult and negative expression through words 'hard dom â€¢ single â€¢ lesbian'.\n",
      "Classify whether the sentence is hate speech, offensive, or normal and provide a list of target communities.\n",
      "\n",
      "### Sentence:\n",
      "new to nsfw twitter â€¢ <number> â€¢ us â€¢ soft hard dom â€¢ single â€¢ lesbian â€¢ she her lt <number> rt for mutual\n",
      "\n",
      "### Classification:\n",
      " normal \n",
      "\n",
      "### Target Communities:\n",
      " Homosexual \n",
      "\n",
      "### Explanation:\n",
      " None \n",
      "\n",
      "\n",
      "hate speech ['Homosexual'] this sentence is hate speech because it uses discriminatory, prejorative, and violent language against Homosexual through words 'cuck' and 'gay'.\n",
      "Classify whether the sentence is hate speech, offensive, or normal and provide a list of target communities.\n",
      "\n",
      "### Sentence:\n",
      "glad to hear do not be a sheltered cuck for any side lefties are gay as fuck but gayness is contagious to the right as well but i think you already grasp that\n",
      "\n",
      "### Classification:\n",
      " offensive \n",
      "\n",
      "### Target Communities:\n",
      " Homosexual \n",
      "\n",
      "### Explanation:\n",
      " this sentence is offensive because it contains insult and negative expression against Homosexual through words 'cuck' and 'gay as fuck'. \n",
      "\n",
      "\n",
      "offensive ['Jewish'] this sentence is offensive because it contains insult and negative expression against Jewish through words 'genocide jews'.\n",
      "Classify whether the sentence is hate speech, offensive, or normal and provide a list of target communities.\n",
      "\n",
      "### Sentence:\n",
      "concentrate on white genocide jews elites corrupt politicians etc and leave christianity alone\n",
      "\n",
      "### Classification:\n",
      " offensive \n",
      "\n",
      "### Target Communities:\n",
      " Jewish \n",
      "\n",
      "### Explanation:\n",
      " this sentence is offensive because it contains insult and negative expression against Jewish through words 'white genocide jews elites'. \n",
      "\n",
      "\n",
      "hate speech ['African', 'Islam'] this sentence is hate speech because it uses discriminatory, prejorative, and violent language against African and Islam through words 'pack raped by a gang of muzzos' and 'hope she gets aids'.\n",
      "Classify whether the sentence is hate speech, offensive, or normal and provide a list of target communities.\n",
      "\n",
      "### Sentence:\n",
      "merkel needs to be pack raped by a gang of muzzos then ass fucked by african niggers i hope she gets aids and dies in pain\n",
      "\n",
      "### Classification:\n",
      " hate speech \n",
      "\n",
      "### Target Communities:\n",
      " African, Islam \n",
      "\n",
      "### Explanation:\n",
      " this sentence is hate speech because it uses discriminatory, prejorative, and violent language against African and Islam through words 'pack raped by a gang of muzzos then ass fucked by african niggers'. \n",
      "\n",
      "\n",
      "offensive [] this sentence is offensive because it contains insult and negative expression through words 'bitch'.\n",
      "Classify whether the sentence is hate speech, offensive, or normal and provide a list of target communities.\n",
      "\n",
      "### Sentence:\n",
      "these ain t triple bitch these white lows\n",
      "\n",
      "### Classification:\n",
      " normal \n",
      "\n",
      "### Target Communities:\n",
      " None \n",
      "\n",
      "### Explanation:\n",
      " None \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, example in enumerate(hf_eval_dataset):\n",
    "        # Get the sentence, true label, and true targets\n",
    "        sentence_tokens = example[\"post_tokens\"]\n",
    "        sentence = \" \".join(sentence_tokens)\n",
    "        true_label_id = example[\"label\"]\n",
    "        true_label = label_mapping.get(true_label_id, 'Unknown')\n",
    "        explanations = example[\"explanation\"]\n",
    "        if true_label == \"normal\":\n",
    "            continue\n",
    "        true_targets = example[\"target\"]\n",
    "        print(true_label, true_targets, explanations)\n",
    "\n",
    "        # Generate the prompt\n",
    "        prompt = generate_prompt(sentence)\n",
    "\n",
    "        # Tokenize the input\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "\n",
    "        # Generate the prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = peft_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=100, #Control here\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                stopping_criteria = stopping_criteria,\n",
    "                temperature=0.7,\n",
    "                num_beams=5,\n",
    "                early_stopping=True\n",
    "            )\n",
    "\n",
    "        # Decode the generated output\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(generated_text)\n",
    "        # all_generated_texts.append(generated_text)\n",
    "        # print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec051503-14e8-4a34-81a7-23a806b0e9e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_prompt(sentence):\n",
    "    prompt = (f\"Classify whether the sentence is hate speech, offensive, or normal and provide a list of target communities.\\n\\n\"\n",
    "            f\"### Sentence:\\n{sentence}\\n\\n\")\n",
    "    return prompt\n",
    "\n",
    "\n",
    "label_mapping = {0: \"hate speech\", 1: \"normal\", 2: \"offensive\", 3: \"undecided\"}\n",
    "reverse_label_mapping = {v.lower(): k for k, v in label_mapping.items()}\n",
    "\n",
    "def extract_targets(sentence):\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    # Step 1: Remove '###' at the end if present\n",
    "    sentence = re.sub(r'###$', '', sentence)\n",
    "\n",
    "    # Step 2: Remove special characters except commas\n",
    "    sentence = re.sub(r'[^A-Za-z0-9, ]+', '', sentence)\n",
    "\n",
    "    # Step 3: Remove extra whitespace\n",
    "    sentence = ' '.join(sentence.split())\n",
    "    \n",
    "    targets = sentence.split(\",\")\n",
    "    if len(targets)  < 1:\n",
    "        return []\n",
    "    cleaned_targets = [target.strip() for target in targets]\n",
    "    return cleaned_targets\n",
    "    \n",
    "def extract_output(generated_text):\n",
    "    \n",
    "    \"\"\"\n",
    "    Extracts the classification label from the generated text.\n",
    "    Assumes the label is one of the predefined classes.\n",
    "    \"\"\"\n",
    "    label = None\n",
    "    targets = []\n",
    "    # Define a regex pattern to capture the label\n",
    "    parts = generated_text.split(\"### Classification:\")\n",
    "    if len(parts) > 1:\n",
    "        classification_text = parts[1].strip()\n",
    "        sub_parts = classification_text.split(\"### Target Communities:\")\n",
    "        if len(sub_parts) > 1:\n",
    "            targets = extract_targets(sub_parts[1])\n",
    "        pattern = r\"\\b(hate speech|offensive|normal|undecided)\\b\"\n",
    "        match = re.search(pattern, sub_parts[0].lower())\n",
    "        if match:\n",
    "            label = match.group(1)\n",
    "    return label, targets  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e034642-fbca-4558-8baf-352a7625b4d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# Define custom stopping criterion that halts when \"###\" appears again after \"Target communities\"\n",
    "class StopOnHashCriteriaAfterTarget(StoppingCriteria):\n",
    "    def __init__(self, stop_token, after_token):\n",
    "        self.stop_token = stop_token.lower()\n",
    "        self.after_token = after_token.lower()\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        # Decode generated token IDs back to a readable string\n",
    "        generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True).lower()\n",
    "        \n",
    "        # Stop if \"###\" appears after \"Target communities\"\n",
    "        if self.after_token in generated_text:\n",
    "            return self.stop_token in generated_text.split(self.after_token)[-1]  # Only consider \"###\" after \"Target communities\"\n",
    "        return False\n",
    "\n",
    "# Create stopping criteria\n",
    "stopping_criteria = StoppingCriteriaList([StopOnHashCriteriaAfterTarget(\"###\", \"Target Communities:\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71394a08-949c-4b24-b8cc-f8f6f8ed8082",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Examples:   0%|          | 0/100 [00:00<?, ?it/s]/rhome/sfaa2021/miniconda3/envs/thesis/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "Processing Examples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [04:55<00:00,  2.96s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # For progress bar\n",
    "import csv\n",
    "\n",
    "# Initialize lists to store predictions and true labels\n",
    "all_predicted_labels = []\n",
    "all_predicted_targets=[]\n",
    "all_true_labels = []\n",
    "all_true_targets = []\n",
    "all_generated_texts = []\n",
    "all_prompts = []\n",
    "\n",
    "output_file_path=\"inferences/mistral_attention_all_100_epoch\"\n",
    "# Move model to device and set to evaluation mode\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "peft_model.to(device)\n",
    "peft_model.eval()\n",
    "\n",
    "with open(output_file_path, mode=\"w\", newline='', encoding='utf-8') as csv_file:\n",
    "    fieldnames = [\"Index\", \"Generated_Text\", \"True_Label\", \"Predicted_Label\", \"True_Targets\", \"Predicted_Targets\"]\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    \n",
    "    writer.writeheader()\n",
    "    \n",
    "    for idx, example in enumerate(tqdm(hf_eval_dataset, desc=\"Processing Examples\")):\n",
    "        # Get the sentence, true label, and true targets\n",
    "        sentence_tokens = example[\"post_tokens\"]\n",
    "        sentence = \" \".join(sentence_tokens)\n",
    "        true_label_id = example[\"label\"]\n",
    "        true_label = label_mapping.get(true_label_id, 'Unknown')\n",
    "        true_targets = example[\"target\"]\n",
    "\n",
    "        # Generate the prompt\n",
    "        prompt = generate_prompt(sentence)\n",
    "        all_prompts.append(prompt)\n",
    "\n",
    "        # Tokenize the input\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "\n",
    "        # Generate the prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = peft_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=50,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                stopping_criteria = stopping_criteria,\n",
    "                temperature=0.7,\n",
    "                num_beams=5,\n",
    "                early_stopping=True\n",
    "            )\n",
    "\n",
    "        # Decode the generated output\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        all_generated_texts.append(generated_text)\n",
    "        # print(generated_text)\n",
    "\n",
    "        # Extract output from generated text\n",
    "        predicted_label, predicted_targets = extract_output(generated_text)\n",
    "        # print(predicted_label_text)\n",
    "        if predicted_label:\n",
    "            predicted_label_id = reverse_label_mapping.get(predicted_label.lower(), -1)\n",
    "            # print(predicted_label_id)\n",
    "        else:\n",
    "            predicted_label_id = -1  # Assign -1 if parsing failed\n",
    "        \n",
    "        if predicted_targets == []:\n",
    "            predicted_targets = [\"none\"]\n",
    "\n",
    "        # Append predictions and true labels\n",
    "        all_predicted_labels.append(predicted_label_id)\n",
    "        all_true_labels.append(true_label_id)\n",
    "        \n",
    "        # Append predicted and true targets\n",
    "        all_predicted_targets.append(predicted_targets)\n",
    "        all_true_targets.append(true_targets)\n",
    "        \n",
    "        writer.writerow({'Index': idx, 'Generated_Text': generated_text, 'True_Label': true_label_id, 'Predicted_Label': predicted_label_id, 'True_Targets': true_targets, 'Predicted_Targets': predicted_targets})\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd76fac9-6abc-40d5-8c41-afb70b196feb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_predicted_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 162\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_precision\u001b[39m\u001b[38;5;124m'\u001b[39m: average_precision,\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_recall\u001b[39m\u001b[38;5;124m'\u001b[39m: average_recall,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro_f1_score\u001b[39m\u001b[38;5;124m'\u001b[39m: macro_f1_score,\n\u001b[1;32m    159\u001b[0m     }\n\u001b[1;32m    161\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mafrican\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marab\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masian\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaucasian\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhispanic\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuddhism\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhindu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mislam\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjewish\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmen\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwomen\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheterosexual\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgay\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindigenous\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrefugee\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimmigrant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28mprint\u001b[39m(evaluate_labels(\u001b[43mall_predicted_labels\u001b[49m, all_true_labels))\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28mprint\u001b[39m(evaluate_targets(all_predicted_targets, all_true_targets, labels))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_predicted_labels' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def evaluate_labels(all_predicted_labels, all_true_labels):\n",
    "    # Filter out invalid predictions (where label_id == -1)\n",
    "    valid_indices = [i for i, pred in enumerate(all_predicted_labels) if pred != -1]\n",
    "    valid_predictions = [all_predicted_labels[i] for i in valid_indices]\n",
    "    valid_true_labels = [all_true_labels[i] for i in valid_indices]\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = accuracy_score(valid_true_labels, valid_predictions)\n",
    "    print(f\"Accuracy on valid predictions: {accuracy:.2f}\")\n",
    "\n",
    "    labels = [0, 1, 2, 3]  # All possible label indices\n",
    "    target_names = [label_mapping[i] for i in labels]\n",
    "\n",
    "    # Compute detailed classification report\n",
    "    output = \"\\nClassification Report:\\n\\n\"\n",
    "    output += classification_report(\n",
    "        valid_true_labels,\n",
    "        valid_predictions,\n",
    "        labels=labels,\n",
    "        target_names=target_names,\n",
    "        zero_division=0  # Prevents division by zero warnings\n",
    "    )\n",
    "    return output\n",
    "    \n",
    "\n",
    "def evaluate_targets(all_predicted_targets, all_true_targets, labels):\n",
    "    \"\"\"\n",
    "    Evaluate precision, recall, and F1 score for each sample and overall.\n",
    "\n",
    "    Args:\n",
    "        all_predicted_targets (list of list of str): Predicted labels per sample.\n",
    "        all_true_labels (list of list of str): True labels per sample.\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains per-sample metrics, average per-sample metrics, and overall metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    total_true_positives = 0\n",
    "    total_false_positives = 0\n",
    "    total_false_negatives = 0\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    num_examples = len(all_true_targets)\n",
    "    \n",
    "    # Initialize counts for each label\n",
    "    label_true_positives = {label.lower(): 0 for label in labels}\n",
    "    label_false_positives = {label.lower(): 0 for label in labels}\n",
    "    label_false_negatives = {label.lower(): 0 for label in labels}\n",
    "\n",
    "    # Iterate over each sample\n",
    "    for predicted, true in zip(all_predicted_targets, all_true_targets):\n",
    "        # Convert labels to lower case to handle case insensitivity\n",
    "        predicted_set = set(label.lower() for label in predicted)\n",
    "        true_set = set(label.lower() for label in true)\n",
    "\n",
    "        # Calculate true positives, false positives, and false negatives\n",
    "        true_positives = len(true_set & predicted_set)\n",
    "        false_positives = len(predicted_set - true_set)\n",
    "        false_negatives = len(true_set - predicted_set)\n",
    "\n",
    "        # Update total counts\n",
    "        total_true_positives += true_positives\n",
    "        total_false_positives += false_positives\n",
    "        total_false_negatives += false_negatives\n",
    "\n",
    "        # Calculate precision and recall for this example\n",
    "        precision = (\n",
    "            true_positives / (true_positives + false_positives)\n",
    "            if (true_positives + false_positives) > 0\n",
    "            else 0\n",
    "        )\n",
    "        recall = (\n",
    "            true_positives / (true_positives + false_negatives)\n",
    "            if (true_positives + false_negatives) > 0\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "        # Accumulate precision and recall for average calculation\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        \n",
    "        for label in labels:\n",
    "            label = label.lower()\n",
    "            if label in true_set and label in predicted_set:\n",
    "                # True Positive\n",
    "                label_true_positives[label] += 1\n",
    "            elif label not in true_set and label in predicted_set:\n",
    "                # False Positive\n",
    "                label_false_positives[label] += 1\n",
    "            elif label in true_set and label not in predicted_set:\n",
    "                # False Negative\n",
    "                label_false_negatives[label] += 1\n",
    "            # else:\n",
    "            #     True Negative (not used in precision/recall calculations)\n",
    "\n",
    "\n",
    "    # Calculate average precision and recall over all examples\n",
    "    average_precision = total_precision / num_examples if num_examples > 0 else 0\n",
    "    average_recall = total_recall / num_examples if num_examples > 0 else 0\n",
    "\n",
    "    # Calculate overall precision and recall across all examples\n",
    "    overall_precision = (\n",
    "        total_true_positives / (total_true_positives + total_false_positives)\n",
    "        if (total_true_positives + total_false_positives) > 0\n",
    "        else 0\n",
    "    )\n",
    "    overall_recall = (\n",
    "        total_true_positives / (total_true_positives + total_false_negatives)\n",
    "        if (total_true_positives + total_false_negatives) > 0\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    # Calculate F1 score from overall precision and recall\n",
    "    f1_score = (\n",
    "        2 * overall_precision * overall_recall / (overall_precision + overall_recall)\n",
    "        if (overall_precision + overall_recall) > 0\n",
    "        else 0\n",
    "    )\n",
    "    \n",
    "    # Calculate precision, recall, and F1 score for each label\n",
    "    per_label_precision = {}\n",
    "    per_label_recall = {}\n",
    "    per_label_f1 = {}\n",
    "\n",
    "    for label in labels:\n",
    "        label = label.lower()\n",
    "        tp = label_true_positives[label]\n",
    "        fp = label_false_positives[label]\n",
    "        fn = label_false_negatives[label]\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        per_label_precision[label] = precision\n",
    "        per_label_recall[label] = recall\n",
    "        per_label_f1[label] = f1\n",
    "    \n",
    "    # Calculate macro-averaged precision, recall, and F1 score\n",
    "    macro_precision = sum(per_label_precision.values()) / len(labels)\n",
    "    macro_recall = sum(per_label_recall.values()) / len(labels)\n",
    "    macro_f1_score = sum(per_label_f1.values()) / len(labels)\n",
    "        \n",
    "\n",
    "    return {\n",
    "        'average_precision': average_precision,\n",
    "        'average_recall': average_recall,\n",
    "        'overall_precision': overall_precision,\n",
    "        'overall_recall': overall_recall,\n",
    "        'f1_score': f1_score,\n",
    "        'per_label_precision': per_label_precision,\n",
    "        'per_label_recall': per_label_recall,\n",
    "        'per_label_f1': per_label_f1,\n",
    "        'macro_precision': macro_precision,\n",
    "        'macro_recall': macro_recall,\n",
    "        'macro_f1_score': macro_f1_score,\n",
    "    }\n",
    "\n",
    "labels = [\"african\", \"arab\", \"asian\", \"caucasian\", \"christian\", \"hispanic\", \"buddhism\", \"hindu\", \"islam\", \"jewish\", \"men\", \"women\", \"heterosexual\", \"homosexual\", \"indigenous\", \"refugee\", \"immigrant\", \"disability\", \"none\"]\n",
    "print(evaluate_labels(all_predicted_labels, all_true_labels))\n",
    "print(evaluate_targets(all_predicted_targets, all_true_targets, labels))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8de7af24-8ff6-4e05-9cf1-2ee1a723cbc0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Identify failed predictions (incorrect or parsing failed)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m failed_indices \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mall_predictions\u001b[49m)) \u001b[38;5;28;01mif\u001b[39;00m all_predictions[i] \u001b[38;5;241m!=\u001b[39m all_true_labels[i]]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNumber of failed predictions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(failed_indices)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Display a few failed examples\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_predictions' is not defined"
     ]
    }
   ],
   "source": [
    "# # Identify failed predictions (incorrect or parsing failed)\n",
    "# failed_indices = [i for i in range(len(all_predictions)) if all_predictions[i] != all_true_labels[i]]\n",
    "\n",
    "# print(f\"\\nNumber of failed predictions: {len(failed_indices)}\")\n",
    "\n",
    "# # Display a few failed examples\n",
    "# num_examples_to_show = 5\n",
    "# print(f\"\\nDisplaying first {num_examples_to_show} failed predictions:\\n\")\n",
    "# for i in range(min(num_examples_to_show, len(failed_indices))):\n",
    "#     idx = failed_indices[i]\n",
    "#     print(f\"Example {idx + 1}:\")\n",
    "#     print(f\"Prompt:\\n{all_prompts[idx]}\")\n",
    "#     print(f\"Generated Prediction:\\n{all_generated_texts[idx]}\")\n",
    "#     print(f\"True Label: {label_mapping.get(all_true_labels[idx], 'Unknown')}\")\n",
    "#     predicted_label = label_mapping.get(all_predictions[idx], 'Unknown') if all_predictions[idx] != -1 else 'Parsing Failed'\n",
    "#     print(f\"Predicted Label: {predicted_label}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_env",
   "language": "python",
   "name": "hf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
