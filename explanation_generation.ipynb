{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9d7b1a9-3367-471f-b280-dad3ba841762",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "440231533c2140ceb87ded201130dd5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from transformers import pipeline\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm  # For progress bar\n",
    "import csv\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\n",
    "from huggingface_hub import HfApi, HfFolder\n",
    "from peft import PeftModel\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", quantization_config=bnb_config, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Saef/mistral_attention_second_epoch\", quantization_config=bnb_config, device_map=\"auto\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Saef/mistral_attention_second_epoch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae4913d0-d1e7-4944-b26a-e3c3839182a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOS token ID: 2\n",
      "BOS token ID: 1\n",
      "PAD token ID: None\n",
      "EOS token ID (tokenizer): 2\n",
      "BOS token ID (tokenizer): 1\n",
      "PAD token ID (tokenizer): 1\n"
     ]
    }
   ],
   "source": [
    "# From the model config\n",
    "print(\"EOS token ID:\", model.config.eos_token_id)\n",
    "print(\"BOS token ID:\", model.config.bos_token_id)\n",
    "print(\"PAD token ID:\", model.config.pad_token_id)\n",
    "\n",
    "# Or from the tokenizer\n",
    "print(\"EOS token ID (tokenizer):\", tokenizer.eos_token_id)\n",
    "print(\"BOS token ID (tokenizer):\", tokenizer.bos_token_id)\n",
    "print(\"PAD token ID (tokenizer):\", tokenizer.pad_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ab40722-cd7f-4a70-99d1-ef66f0b5dcc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain why the given sentence is considered hate speech against Asians. \n",
      "\n",
      "### Example Sentence:\n",
      "I wish I could make all these chinese motherfuckers disappear from our planet.\n",
      "\n",
      "### Explanation:\n",
      "The sentence is hate speech because it expresses hatred toward Chinese through words, motherfuckers and disapear. \n",
      "\n",
      "### Example Sentence:\n",
      "I dont understand why all japanese have flat boobs—maybe because they are once colonized.\n",
      "\n",
      "### Explanation:\n",
      "The sentence is offensive because it expressed negatively about the Japanese through the words. flat boobs and colonized.\n",
      "\n",
      "### Sentence:\n",
      "i love those liberaltears the muslim ban and the recognition of jerusalem as israel capital shows arabs antisemitism causing more redpillingofthenormies i fucking love you president trump just do not be a cuck to liberals donald <user>\n",
      "\n",
      "### Explanation:\n",
      "The sentence is hate speech against Asians because it expresses hatred toward Asian through words, redpilling, normies and liberaltears.\n",
      "\n",
      "### Sentence:\n",
      "The problem with feminism is that it is the only movement where the men are oppressors, the women are victims, and the oppressors are the ones who are supposed to fix the oppression.\n",
      "\n",
      "### Explanation:\n",
      "The sentence is hate speech against women\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer = tokenizer, \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "test_sentence = \"Thee Ching-Chongs out here know nothing but to sell their pussies for their colonizers like white men and it makes sense why white men would want to commit atrocities against such whores.\"\n",
    "test_sentence_2 = \"these ain t triple bitch these white lows\"\n",
    "test_sentence_3 = \"the bengals head coach is a fucking retard\"\n",
    "test_sentence_4 = \"Asian hate crime is on the rise, but it seems the president is doing nothing to tackle it.\"\n",
    "test_sentence_5 = \"lol im just laughing my ass off at you for thinking that white identity actually means something i guess then that you identify with the hollywood white commies\"\n",
    "test_sentence_6 = \"i love those liberaltears the muslim ban and the recognition of jerusalem as israel capital shows arabs antisemitism causing more redpillingofthenormies i fucking love you president trump just do not be a cuck to liberals donald <user>\"\n",
    "\n",
    "test_prompt = (\n",
    "    f\"Classify whether the sentence is hate speech, offensive, or normal. Then, provide a list of target communities included in the sentence. Finally, provide an explanation for the classification. \\n\\n\"\n",
    "    # One-shot example\n",
    "    f\"### Example Sentence:\\n\"\n",
    "    f\"I wish I could make all these chinese motherfuckers disappear from our planet.\\n\\n\"\n",
    "    f\"### Classification:\\nHate Speech\\n\\n\"\n",
    "    f\"### Target Communities:\\nChinese\\n\\n\"\n",
    "    f\"### Explanation:\\nThe sentence is hate speech because it expresses hatred toward Chinese through words, motherfuckers and disapear. \\n\\n\"\n",
    "    \n",
    "    f\"### Example Sentence:\\n\"\n",
    "    f\"I dont understand why all japanese have flat boobs—maybe because they are once colonized.\\n\\n\"\n",
    "    f\"### Classification:\\nOffensive\\n\\n\"\n",
    "    f\"### Target Communities:\\nJapanese\\n\\n\"\n",
    "    f\"### Explanation:\\nThe sentence is offensive because it expressed negatively about the Japanese through the words. flat boobs and colonized.\\n\\n\"\n",
    "    \n",
    "    # Your test sentence\n",
    "    f\"### Sentence:\\n{test_sentence_3}\\n\\n\"\n",
    ")\n",
    "\n",
    "test_prompt_2 = (\n",
    "    f\"Explain why the given sentence is considered hate speech against Asians. \\n\\n\"\n",
    "    \n",
    "    f\"### Example Sentence:\\n\"\n",
    "    f\"I wish I could make all these chinese motherfuckers disappear from our planet.\\n\\n\"\n",
    "    f\"### Explanation:\\nThe sentence is hate speech because it expresses hatred toward Chinese through words, motherfuckers and disapear. \\n\\n\"\n",
    "    \n",
    "    f\"### Example Sentence:\\n\"\n",
    "    f\"I dont understand why all japanese have flat boobs—maybe because they are once colonized.\\n\\n\"\n",
    "    f\"### Explanation:\\nThe sentence is offensive because it expressed negatively about the Japanese through the words. flat boobs and colonized.\\n\\n\"\n",
    "    \n",
    "    # Your test sentence\n",
    "    f\"### Sentence:\\n{test_sentence_6}\\n\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "sequences = pipe(\n",
    "    test_prompt_2,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=100, \n",
    "    temperature=0.7, \n",
    "    top_k=50, \n",
    "    top_p=0.95,\n",
    "    num_return_sequences=1,\n",
    ")\n",
    "print(sequences[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18234543-8f24-499e-9ae7-2ca4556111a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Explain why the given sentence is considered hate speech against Asians. \n",
      "\n",
      "### Example Sentence:\n",
      "I wish I could make all these chinese motherfuckers disappear from our planet.\n",
      "\n",
      "### Explanation:\n",
      "The sentence is hate speech because it expresses hatred toward Chinese through words, motherfuckers and disapear. \n",
      "\n",
      "### Example Sentence:\n",
      "I dont understand why all japanese have flat boobs—maybe because they are once colonized.\n",
      "\n",
      "### Explanation:\n",
      "The sentence is offensive because it expressed negatively about the Japanese through the words. flat boobs and colonized.\n",
      "\n",
      "### Sentence:\n",
      "i love those liberaltears the muslim ban and the recognition of jerusalem as israel capital shows arabs antisemitism causing more redpillingofthenormies i fucking love you president trump just do not be a cuck to liberals donald <user>\n",
      "\n",
      "### Explanation:\n",
      "The sentence is hate speech because it expresses hatred toward Muslims through words, liberaltears and antisemitism.\n",
      "\n",
      "### Sentence:\n",
      "I wish I could make all these chinese motherfuckers disappear from our planet.\n",
      "\n",
      "### Explanation:\n",
      "The sentence is hate speech because it expresses hatred toward Chinese through words, motherfuckers and disapear.\n",
      "\n",
      "### Sentence:\n",
      "I dont understand why\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(test_prompt_2, return_tensors=\"pt\", max_length=512, truncation=True).to(model.device)\n",
    "\n",
    "# Generate the prediction with correct padding and EOS token handling\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    # do_sample = True,\n",
    "    # top_k = 50,\n",
    "    # top_p = 0.95,\n",
    "    repetition_penalty = 1.0,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=model.config.eos_token_id,\n",
    "    pad_token_id=model.config.pad_token_id,\n",
    "    bos_token_id=model.config.bos_token_id,\n",
    "    temperature=0.7,\n",
    "    early_stopping = True\n",
    ")\n",
    "\n",
    "# Decode the generated output to human-readable text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated text:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98bcf825-f07b-4ddd-99bc-0f229ce1e2a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated token IDs: tensor([    1, 13702,   426,  2079,   272,  2078, 12271,   349,  4525,  7665,\n",
      "         8666,  1835,  1136,  3693, 28723, 28705,    13,    13, 27332, 16693,\n",
      "          318,   308,   636, 28747,    13, 28737,  5138,   315,   829,  1038,\n",
      "          544,  1167,   484,  5965,  3057, 28722,  1384,   404, 22691,   477,\n",
      "          813,  9873, 28723,    13,    13, 27332,  1529, 11009,   352, 28747,\n",
      "           13,  1014, 12271,   349,  7665,  8666,  1096,   378,  4072,   274,\n",
      "        25944,  4112,  6707,  1059,  3085, 28725,  3057, 28722,  1384,   404,\n",
      "          304,   704,   377,   644, 28723, 28705,    13,    13, 27332, 16693,\n",
      "          318,   308,   636, 28747,    13, 28737,  7286,  2380,  2079,   544,\n",
      "          461,  4209,  3368,   506,  7007,  1359,  8081, 28821, 22817,  1096,\n",
      "          590,   460,  2327,  8039,  1332, 28723,    13,    13, 27332,  1529,\n",
      "        11009,   352, 28747,    13,  1014, 12271,   349, 17381,  1096,   378,\n",
      "        11558,  3444,  6308,   684,   272,  8092,  1059,   272,  3085, 28723,\n",
      "         7007,  1359,  8081,   304,  8039,  1332, 28723,    13,    13, 27332,\n",
      "          318,   308,   636, 28747,    13, 28710,  2016,  1395, 14987,   424,\n",
      "         1168,   272,  2009,  2788,  8743,   304,   272, 13828,   302, 22752,\n",
      "        17616,   390,   349,  5815,  5565,  4370,   597,  4737,  2725,   278,\n",
      "        12265,  1443, 13098,   680,  2760, 28720,  8317, 24485,   540,   552,\n",
      "          497,   613,  7714,  2016,   368,  4951, 22945,   776,   511,   459,\n",
      "          347,   264,   277,  1384,   298,  7172,   973,   949,  3165,   523,\n",
      "         1838, 28767,    13,    13, 27332,  1529, 11009,   352, 28747,    13,\n",
      "         1014, 12271,   349,  7665,  8666,  1096,   378,  4072,   274, 25944,\n",
      "         4112, 21731,  1059,  3085, 28725, 14987,   424,  1168,   304,  2725,\n",
      "          278, 12265,  1443, 28723,    13,    13, 27332,   318,   308,   636,\n",
      "        28747,    13, 28737,  5138,   315,   829,  1038,   544,  1167,   484,\n",
      "         5965,  3057, 28722,  1384,   404, 22691,   477,   813,  9873, 28723,\n",
      "           13,    13, 27332,  1529, 11009,   352, 28747,    13,  1014, 12271,\n",
      "          349,  7665,  8666,  1096,   378,  4072,   274, 25944,  4112,  6707,\n",
      "         1059,  3085, 28725,  3057, 28722,  1384,   404,   304,   704,   377,\n",
      "          644, 28723,    13,    13, 27332,   318,   308,   636, 28747,    13,\n",
      "        28737,  7286,  2380,  2079], device='cuda:0')\n",
      "EOS token ID: 2\n",
      "EOS token was NOT generated.\n"
     ]
    }
   ],
   "source": [
    "# # Ensure eos_token_id is passed during generation\n",
    "# outputs = base_model.generate(\n",
    "#     **inputs,\n",
    "#     max_new_tokens=50,  # Limit the number of generated tokens\n",
    "#     eos_token_id=base_tokenizer.eos_token_id,  # Ensure EOS token is recognized\n",
    "#     pad_token_id=base_tokenizer.pad_token_id   # Handle padding properly\n",
    "# )\n",
    "\n",
    "# Print the generated token IDs (raw output before decoding)\n",
    "print(\"Generated token IDs:\", outputs[0])\n",
    "\n",
    "# Compare with the EOS token ID\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "print(f\"EOS token ID: {eos_token_id}\")\n",
    "\n",
    "# Check if the EOS token ID is in the output\n",
    "if eos_token_id in outputs[0]:\n",
    "    print(\"EOS token was generated.\")\n",
    "else:\n",
    "    print(\"EOS token was NOT generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5313428b-fb51-48c9-9d06-103fcd9ba8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from checkpoints/mistral_attention/checkpoint-640...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rhome/sfaa2021/miniconda3/envs/hf_env/lib/python3.10/site-packages/peft/utils/save_and_load.py:227: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d82548bf0f014f3cb74595a9e3b3a252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/602M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ffc034c404400d9ca59aecf4b3a64a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Saef/mistral_attention_second_epoch/commit/30cb506b0cee1d401423d3267f882dcc5c65f466', commit_message='Upload tokenizer', commit_description='', oid='30cb506b0cee1d401423d3267f882dcc5c65f466', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Saef/mistral_attention_second_epoch', endpoint='https://huggingface.co', repo_type='model', repo_id='Saef/mistral_attention_second_epoch'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterate through each checkpoint\n",
    "\n",
    "checkpoint_path = \"checkpoints/mistral_attention/checkpoint-640\"\n",
    "\n",
    "\n",
    "print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
    "\n",
    "# Load PEFT model with the checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "model = PeftModel.from_pretrained(base_model, checkpoint_path)\n",
    "\n",
    "# Push your PEFT model (adapters)\n",
    "model.push_to_hub(\"Saef/mistral_attention_second_epoch\")\n",
    "\n",
    "# # Push the tokenizer as well\n",
    "tokenizer.push_to_hub(\"Saef/mistral_attention_second_epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3964a84d-3b20-47f3-aca8-8b0d533908b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_env",
   "language": "python",
   "name": "hf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
