{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'annotators', 'rationales', 'post_tokens'],\n",
      "        num_rows: 15383\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'annotators', 'rationales', 'post_tokens'],\n",
      "        num_rows: 1922\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'annotators', 'rationales', 'post_tokens'],\n",
      "        num_rows: 1924\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#Load the dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "import json\n",
    "\n",
    "# Load the HateXplain dataset\n",
    "dataset = load_dataset(\"../hatexplain/hatexplain.py\")\n",
    "\n",
    "# Inspect the dataset\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '23107796_gab', 'annotators': {'label': [0, 2, 2], 'annotator_id': [203, 204, 233], 'target': [['Hindu', 'Islam'], ['Hindu', 'Islam'], ['Hindu', 'Islam', 'Other']]}, 'rationales': [[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'post_tokens': ['u', 'really', 'think', 'i', 'would', 'not', 'have', 'been', 'raped', 'by', 'feral', 'hindu', 'or', 'muslim', 'back', 'in', 'india', 'or', 'bangladesh', 'and', 'a', 'neo', 'nazi', 'would', 'rape', 'me', 'as', 'well', 'just', 'to', 'see', 'me', 'cry']}\n"
     ]
    }
   ],
   "source": [
    "train_data = dataset[\"train\"]\n",
    "val_data = dataset[\"validation\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "print(train_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: 15383}\n",
      "count_mistmatch 0\n"
     ]
    }
   ],
   "source": [
    "#Focus on train data for now\n",
    "#Observe the train data\n",
    "\n",
    "#Count the number of annotators\n",
    "count_annotators ={}\n",
    "count_mistmatch = 0\n",
    "for data in train_data:\n",
    "    count = len(data['annotators']['annotator_id'])\n",
    "    if count !=len(data[\"annotators\"][\"label\"]) or count !=count != len(data[\"annotators\"][\"target\"]):\n",
    "        count_mistmatch +=1\n",
    "    if count in count_annotators:\n",
    "        count_annotators[count] +=1\n",
    "    else:\n",
    "        count_annotators[count] = 1\n",
    "\n",
    "print(count_annotators)\n",
    "print(\"count_mistmatch\", count_mistmatch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_rationale_match_annotators:  3518\n",
      "count_rational_not_match_annotators 11865\n",
      "{2: 5614, 3: 3518, 0: 6251}\n"
     ]
    }
   ],
   "source": [
    "#count an item where the number of annotators don't match the number of rationales\n",
    "count_rationale_match_annotators = 0\n",
    "count_rational_not_match_annotators = 0\n",
    "count_rationales = {}\n",
    "for data in train_data:\n",
    "    count = len(data['rationales'])\n",
    "    if count in count_rationales:\n",
    "        count_rationales[count] += 1\n",
    "    else:\n",
    "        count_rationales[count] = 1\n",
    "        \n",
    "    if len(data['annotators'][\"label\"]) == count:\n",
    "        count_rationale_match_annotators += 1\n",
    "    else:\n",
    "        count_rational_not_match_annotators += 1\n",
    "        \n",
    "print(\"count_rationale_match_annotators: \", count_rationale_match_annotators)\n",
    "print(\"count_rational_not_match_annotators\", count_rational_not_match_annotators)\n",
    "print(count_rationales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_rationale_normal (all three anotators thought the speech was normal) 4096\n",
      "no_rationale_else (not all through it was normal) 2155\n",
      "no_rationale_abnormal (the majority vote was not normal):  0\n"
     ]
    }
   ],
   "source": [
    "#Are there data labeled as offensive or hate speech but no annotation?\n",
    "no_rationale_data_list = []\n",
    "for data in train_data:\n",
    "    count = len(data[\"rationales\"])\n",
    "    if count == 0:\n",
    "        no_rationale_data_list.append(data[\"annotators\"][\"label\"])\n",
    "\n",
    "no_rationale_normal = []\n",
    "no_rationale_else = []\n",
    "for data in no_rationale_data_list:\n",
    "    if data[0] == data[1] == data[2] ==1: #normal\n",
    "        no_rationale_normal.append(data)\n",
    "    else:\n",
    "        no_rationale_else.append(data)\n",
    "\n",
    "print(\"no_rationale_normal (all three anotators thought the speech was normal)\", len(no_rationale_normal))\n",
    "print(\"no_rationale_else (not all through it was normal)\", len(no_rationale_else))\n",
    "\n",
    "no_rationale_abnormal = []\n",
    "for data in no_rationale_else:\n",
    "    count = 0\n",
    "    for i in data:\n",
    "        if i == 1:\n",
    "            count +=1\n",
    "    if count < 2:\n",
    "        no_rationale_abnormal.append(data)\n",
    "print(\"no_rationale_abnormal (the majority vote was not normal): \", len(no_rationale_abnormal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_hatel_label_match_rationales:  9016\n",
      "count_hatel_label_no_match_rationales 6367\n"
     ]
    }
   ],
   "source": [
    "#check # of rationales vs # of # of hate speech labels\n",
    "count_hate_label_match_rationales =0\n",
    "count_hate_label_no_match_rationales =0\n",
    "for data in train_data:\n",
    "    count_hate_label = data[\"annotators\"][\"label\"].count(0)\n",
    "    count_rationales = len(data[\"rationales\"])\n",
    "    if count_hate_label == count_rationales:\n",
    "        count_hate_label_match_rationales += 1\n",
    "    else:\n",
    "        count_hate_label_no_match_rationales += 1\n",
    "        \n",
    "print(\"count_hate_label_match_rationales: \", count_hate_label_match_rationales)\n",
    "print(\"count_hate_label_no_match_rationales\", count_hate_label_no_match_rationales)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['Refugee'], ['Refugee'], ['None']], [['Arab', 'Caucasian'], ['None'], ['Arab']], [['Women'], ['None'], ['Women']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['Islam', 'Women'], ['Arab', 'Islam', 'Women'], ['Arab', 'Islam']], [['None'], ['African'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['Other'], ['None'], ['Other']], [['None'], ['Women', 'Homosexual'], ['None']], [['None'], ['Other'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['Other'], ['None']], [['None'], ['None'], ['None']], [['None'], ['Women', 'None'], ['Other']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['Islam'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['Caucasian'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['Women'], ['Women'], ['Women']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['Caucasian'], ['None']], [['Other', 'Caucasian'], ['None'], ['None']], [['None'], ['African', 'Asian', 'Caucasian', 'Hispanic'], ['Caucasian']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['Men', 'Women']], [['None'], ['None'], ['None']], [['Other'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['Women'], ['Other'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['Homosexual'], ['None'], ['None']], [['None'], ['None'], ['None']], [['Homosexual', 'Other'], ['Homosexual'], ['Homosexual']], [['Jewish'], ['None'], ['None']], [['None'], ['None'], ['Refugee']], [['None'], ['None'], ['None']], [['Men', 'African'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['Homosexual'], ['Homosexual'], ['None']], [['None'], ['None'], ['None']], [['Homosexual'], ['Homosexual'], ['None']], [['None'], ['None'], ['None']], [['Jewish'], ['Jewish'], ['Jewish']], [['Islam'], ['None'], ['Islam']], [['Women'], ['Women'], ['None']], [['Women'], ['None'], ['Women']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['Caucasian'], ['None']], [['None'], ['None'], ['None']], [['None'], ['Caucasian'], ['None']], [['None'], ['None'], ['None']], [['Women'], ['Women'], ['Women']], [['None'], ['None'], ['None']], [['None'], ['Women'], ['Women']], [['Refugee'], ['Refugee'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['Homosexual'], ['None']], [['None'], ['None'], ['None']], [['Men', 'Women'], ['None'], ['Refugee']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['Refugee'], ['Refugee'], ['Refugee']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['Refugee', 'Other']], [['Other'], ['None'], ['Refugee']], [['None'], ['None'], ['None']]]\n"
     ]
    }
   ],
   "source": [
    "#Check target\n",
    "target_if_normal = []\n",
    "for data in train_data:\n",
    "    labels = data[\"annotators\"][\"label\"]\n",
    "    if labels[0]==labels[1]==labels[2] == 1: #normal\n",
    "        target_if_normal.append(data[\"annotators\"][\"target\"])\n",
    "\n",
    "print(target_if_normal[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's process the training/eval data\n",
    "from collections import Counter\n",
    "\n",
    "def process_easy_data(dataset):\n",
    "    def get_majority_element(lst):\n",
    "        # Count occurrences of each element\n",
    "        count = Counter(lst)\n",
    "        \n",
    "        # Check if there is a majority (i.e., an element that appears at least twice)\n",
    "        if count.most_common(1)[0][1] >= 2:\n",
    "            return count.most_common(1)[0][0]\n",
    "        else:\n",
    "            return 3  # Indicate undecided if no majority\n",
    "        \n",
    "    easy_data =[]\n",
    "    for data in dataset:\n",
    "        labels = data[\"annotators\"]['label']\n",
    "        targets = data[\"annotators\"][\"target\"]\n",
    "        \n",
    "        final_label = get_majority_element(labels)\n",
    "        \n",
    "        final_targets = []\n",
    "        target_dicts = {}\n",
    "        for target in targets:\n",
    "            for word in target:\n",
    "                if word in target_dicts:\n",
    "                    target_dicts[word] += 1\n",
    "                else:\n",
    "                    target_dicts[word] = 1\n",
    "        for dict in target_dicts:\n",
    "            if target_dicts[dict] >=2:\n",
    "                final_targets.append(dict)\n",
    "        \n",
    "        final_data = {}\n",
    "        final_data[\"post_tokens\"] = data[\"post_tokens\"]\n",
    "        final_data[\"label\"] = final_label\n",
    "        final_data[\"target\"] = final_targets\n",
    "        easy_data.append(final_data)\n",
    "    return easy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "easy_train_data length:  15383\n",
      "[{'post_tokens': ['u', 'really', 'think', 'i', 'would', 'not', 'have', 'been', 'raped', 'by', 'feral', 'hindu', 'or', 'muslim', 'back', 'in', 'india', 'or', 'bangladesh', 'and', 'a', 'neo', 'nazi', 'would', 'rape', 'me', 'as', 'well', 'just', 'to', 'see', 'me', 'cry'], 'label': 2, 'target': ['Hindu', 'Islam']}, {'post_tokens': ['the', 'uk', 'has', 'threatened', 'to', 'return', 'radioactive', 'waste', 'to', 'the', 'eu', 'if', 'an', 'agreement', 'cannot', 'be', 'reached', 'can', 'not', 'we', 'keep', 'the', 'radioactive', 'waste', 'and', 'send', 'back', 'all', 'the', 'paki', 'migrants', 'instead'], 'label': 2, 'target': ['Refugee', 'Indian']}, {'post_tokens': ['if', 'english', 'is', 'not', 'imposition', 'then', 'hindi', 'is', 'also', 'not', 'imposition', 'shut', 'up', 'chutiya', 'retards', 'stophindiimposition'], 'label': 2, 'target': ['Other', 'Hindu']}, {'post_tokens': ['no', 'liberal', 'congratulated', 'hindu', 'refugees', 'post', 'cab', 'because', 'they', 'hate', 'hindus'], 'label': 2, 'target': ['Hindu']}, {'post_tokens': ['he', 'said', 'bro', 'even', 'your', 'texts', 'sound', 'redneck', 'whatever', 'the', 'fuck', 'that', 'means', 'üòÇ', 'ü§∑', 'üèª', '\\u200d', '‚ôÇÔ∏è', '<user>'], 'label': 2, 'target': ['Economic']}, {'post_tokens': ['i', 'live', 'and', 'work', 'with', 'many', 'legal', 'mexican', 'immigrants', 'who', 'are', 'great', 'citizens', 'and', 'trump', 'supporters', 'they', 'have', 'no', 'problem', 'with', 'deporting', 'illegals', 'maga'], 'label': 1, 'target': ['Hispanic', 'Refugee']}, {'post_tokens': ['in', 'britain', 'the', 'prime', 'minister', 'a', 'childless', 'woman', 'london', 'mayor', 'a', 'moslem', 'invader', 'in', 'command', 'of', 'london', 'police', 'a', 'lesbian', 'in', 'command', 'of', 'london', 'fire', 'brigade', 'a', 'fat', 'woman', 'scotland', 'first', 'minister', 'a', 'childless', 'woman'], 'label': 0, 'target': ['Women', 'Disability', 'Islam', 'Homosexual']}, {'post_tokens': ['<user>', '<user>', 'there', 'shd', 'be', 'a', 'film', 'called', 'no', 'hindus', 'in', 'kashmir', 'wud', 'be', 'perfect', 'islamic', 'fanaticsm', 'movement', 'in', 'kashmir'], 'label': 1, 'target': ['Hindu', 'Islam']}, {'post_tokens': ['jews', 'defends', 'jews', 'regardless', 'muslims', 'defend', 'muslins', 'regardless', 'christian', 'european', 'attack', 'christians', 'christian', 'people', 'are', 'on', 'their', 'own', 'and', 'jews', 'and', 'muslims', 'have', 'another', 'home', 'we', 'don', 'ffs', 'we', 'do', 'not', 'why', 'can', 'not', 'people', 'understand', 'that', 'it', 'not', 'that', 'difficult'], 'label': 1, 'target': ['Christian', 'Islam']}, {'post_tokens': ['the', 'non', 'partisan', 'congressional', 'budget', 'office', 'cbo', 'has', 'released', 'their', 'report', 'on', 'sens', 'lindsey', 'graham', 'and', 'dick', 'durbin', '<number>', 'the', 'dream', 'act', 'the', 'report', 'shows', 'that', 'their', 'legislation', 'would', 'add', '<number>', 'billion', 'to', 'the', 'deficit', 'over', 'a', 'ten', 'year', 'period', 'and', 'lots', 'more', 'immigrant', 'scum'], 'label': 0, 'target': ['Refugee']}]\n"
     ]
    }
   ],
   "source": [
    "#Processing train data\n",
    "easy_train_data = process_easy_data(train_data)\n",
    "print(\"easy_train_data length: \", len(easy_train_data))\n",
    "print(easy_train_data[:10])\n",
    "# Save list as a JSON file\n",
    "with open('easy_train_data.json', 'w') as f:\n",
    "    json.dump(easy_train_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "easy_eval_data length:  1922\n",
      "[{'post_tokens': ['me', 'getting', 'books', 'from', 'the', 'library', 'about', 'queer', 'cowboys', 'and', 'homoeroticism', 'in', 'cinema', 'instead', 'of', 'anything', 'useful', 'for', 'my', 'thesis'], 'label': 1, 'target': ['None']}, {'post_tokens': ['y', 'si', 'fuera', 'top', '<number>', 'me', 'faltar√≠an', 'how', 'to', 'get', 'away', 'with', 'murder', 'gossip', 'girl', 'the', 'last', 'ship', 'orphan', 'black', 'downton', 'abbey'], 'label': 1, 'target': ['None']}, {'post_tokens': ['<user>', '<user>', '<user>', 'i', 'am', 'a', 'lesbian', 'not', 'queer', 'and', 'quite', 'frankly', 'his', 'disco', 'tits', 'video', 'was', 'repulsive', 'if', 'he', 'thinks', 'being', 'an', 'overweight', 'gay', 'bloke', 'brings', 'out', 'the', 'woman', 'in', 'him', 'then', 'i', 'am', 'offended', 'it', 'pathetic', 'attention', 'seeking'], 'label': 1, 'target': ['None']}, {'post_tokens': ['<user>', 'by', 'tweeting', 'about', 'a', 'civil', 'war', 'trump', 'is', 'sending', 'a', 'message', 'to', 'all', 'his', 'gun', 'toting', 'proud', 'boy', 'nazi', 'white', 'supremacist', 'right', 'wing', 'nutjobs', 'to', 'crawl', 'out', 'of', 'the', 'shadows', 'rebel', 'desperate', 'trump', 'will', 'do', 'dangerous', 'things'], 'label': 1, 'target': ['None']}, {'post_tokens': ['<user>', '<user>', 'you', 'all', 'only', 'caring', 'about', 'their', 'hair', 'if', 'these', 'kids', 'were', 'white', 'it', 'would', 'be', 'different', 'latinos', 'love', 'playing', 'both', 'sides', 'they', 'your', 'homeboy', 'until', 'u', 'say', 'mi', 'amigo', 'or', 'deportation', 'then', 'u', 'a', 'porch', 'monkey', 'i', 'could', 'never'], 'label': 1, 'target': ['None']}, {'post_tokens': ['<user>', 'number', 'of', 'people', 'being', 'born', 'under', 'the', 'poverty', 'line', 'all', 'time', 'high', 'too', 'but', 'i', 'guess', 'let', 'push', 'for', 'some', 'retarded', 'pseudo', 'feminism'], 'label': 1, 'target': ['Women']}, {'post_tokens': ['you', 'bitches', 'are', 'retarded', 'learn', 'your', 'self', 'worth', 'stop', 'trying', 'to', 'come', 'for', 'the', 'female', 'and', 'correct', 'your', 'wack', 'ass', 'man'], 'label': 1, 'target': ['None']}, {'post_tokens': ['<user>', 'argentina', 'still', 'has', 'a', 'big', 'nazi', 'following', 'in', 'some', 'parts', 'the', 'ideology', 'is', 'alive', 'and', 'well', 'there', 'to', 'this', 'day'], 'label': 1, 'target': ['None']}, {'post_tokens': ['queer', 'eye', 'but', 'it', 'straight', 'people', 'making', 'over', 'gays', 'and', 'it', 'conversion', 'therapy'], 'label': 1, 'target': ['None']}, {'post_tokens': ['<user>', '<user>', '<user>', 'i', 'love', 'you'], 'label': 1, 'target': ['None']}]\n"
     ]
    }
   ],
   "source": [
    "#Process eval data\n",
    "easy_eval_data = process_easy_data(val_data)\n",
    "print(\"easy_eval_data length: \", len(easy_eval_data))\n",
    "print(easy_eval_data[:10])\n",
    "\n",
    "# Save list as a JSON file\n",
    "with open('easy_eval_data.json', 'w') as f:\n",
    "    json.dump(easy_eval_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sfaa2021",
   "language": "python",
   "name": "sfaa2021"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13cbab73567c1bd594493c0dbc10dff52949031274552f5498bec08ae58d7886"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
