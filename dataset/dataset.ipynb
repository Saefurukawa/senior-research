{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'annotators', 'rationales', 'post_tokens'],\n",
      "        num_rows: 15383\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'annotators', 'rationales', 'post_tokens'],\n",
      "        num_rows: 1922\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'annotators', 'rationales', 'post_tokens'],\n",
      "        num_rows: 1924\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#Load the dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "import json\n",
    "\n",
    "# Load the HateXplain dataset\n",
    "dataset = load_dataset(\"../hatexplain/hatexplain.py\")\n",
    "\n",
    "# Inspect the dataset\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'annotators', 'rationales', 'post_tokens'],\n",
      "    num_rows: 15383\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'annotators', 'rationales', 'post_tokens'],\n",
      "    num_rows: 1922\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'annotators', 'rationales', 'post_tokens'],\n",
      "    num_rows: 1924\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_data = dataset[\"train\"]\n",
    "val_data = dataset[\"validation\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "print(train_data)\n",
    "print(val_data)\n",
    "print(test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '15473637_gab', 'annotators': {'label': [0, 0, 0], 'annotator_id': [12, 9, 3], 'target': [['Other'], ['Women', 'Islam'], ['Women', 'Islam']]}, 'rationales': [[0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]], 'post_tokens': ['democrats', 'from', 'minnesota', 'leftist', 'muzzie', 'shithole', 'are', 'the', 'dumbest', 'anti', 'americans', 'there', 'are', 'klobuchar', 'one', 'of', 'the', 'reps', 'is', 'a', 'dumb', 'bitch', 'who', 'has', 'a', 'moronic', 'position', 'on', 'the', 'tax', 'bill', 'fuck', 'this', 'stupid', 'woman']}\n"
     ]
    }
   ],
   "source": [
    "for i, row in enumerate(val_data):\n",
    "    post_tokens = row[\"post_tokens\"]\n",
    "    if \"muzzie\" in post_tokens and \"shithole\" in post_tokens:\n",
    "        print(row)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '23107796_gab', 'annotators': {'label': [0, 2, 2], 'annotator_id': [203, 204, 233], 'target': [['Hindu', 'Islam'], ['Hindu', 'Islam'], ['Hindu', 'Islam', 'Other']]}, 'rationales': [[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'post_tokens': ['u', 'really', 'think', 'i', 'would', 'not', 'have', 'been', 'raped', 'by', 'feral', 'hindu', 'or', 'muslim', 'back', 'in', 'india', 'or', 'bangladesh', 'and', 'a', 'neo', 'nazi', 'would', 'rape', 'me', 'as', 'well', 'just', 'to', 'see', 'me', 'cry']}\n",
      "{'id': '9995600_gab', 'annotators': {'label': [2, 2, 0], 'annotator_id': [27, 6, 4], 'target': [['Minority', 'Refugee', 'Indian'], ['Indian'], ['Refugee', 'Islam']]}, 'rationales': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]], 'post_tokens': ['the', 'uk', 'has', 'threatened', 'to', 'return', 'radioactive', 'waste', 'to', 'the', 'eu', 'if', 'an', 'agreement', 'cannot', 'be', 'reached', 'can', 'not', 'we', 'keep', 'the', 'radioactive', 'waste', 'and', 'send', 'back', 'all', 'the', 'paki', 'migrants', 'instead']}\n",
      "{'id': '1227920812235051008_twitter', 'annotators': {'label': [2, 2, 2], 'annotator_id': [209, 203, 222], 'target': [['Other'], ['Hindu'], ['Hindu', 'Other']]}, 'rationales': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]], 'post_tokens': ['if', 'english', 'is', 'not', 'imposition', 'then', 'hindi', 'is', 'also', 'not', 'imposition', 'shut', 'up', 'chutiya', 'retards', 'stophindiimposition']}\n",
      "{'id': '1204931715778543624_twitter', 'annotators': {'label': [2, 2, 2], 'annotator_id': [235, 222, 209], 'target': [['Hindu'], ['Hindu'], ['Hindu']]}, 'rationales': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]], 'post_tokens': ['no', 'liberal', 'congratulated', 'hindu', 'refugees', 'post', 'cab', 'because', 'they', 'hate', 'hindus']}\n",
      "{'id': '1179102559241244672_twitter', 'annotators': {'label': [2, 2, 2], 'annotator_id': [51, 25, 4], 'target': [['Economic'], ['Economic'], ['Caucasian']]}, 'rationales': [[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'post_tokens': ['he', 'said', 'bro', 'even', 'your', 'texts', 'sound', 'redneck', 'whatever', 'the', 'fuck', 'that', 'means', 'üòÇ', 'ü§∑', 'üèª', '\\u200d', '‚ôÇÔ∏è', '<user>']}\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(train_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found at index: 3\n"
     ]
    }
   ],
   "source": [
    "#peak into the sentence\n",
    "sentence_to_find = \"no liberal congratulated hindu refugee post cab because they hate hindus\"\n",
    "for i, row in enumerate(train_data):\n",
    "    sentence = \" \".join(row['post_tokens'])  # Combine tokens into a sentence\n",
    "    if \"hindu\" in sentence and \"liberal\" in sentence and \"congratulated\" in sentence:\n",
    "        print(f\"Found at index: {i}\")\n",
    "        break\n",
    "        # print(test_data[i])\n",
    "        # break\n",
    "else:\n",
    "    print(\"Sentence not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '1178613994371928065_twitter', 'annotators': {'label': [1, 1, 1], 'annotator_id': [83, 3, 51], 'target': [['None'], ['None'], ['None']]}, 'rationales': [], 'post_tokens': ['me', 'getting', 'books', 'from', 'the', 'library', 'about', 'queer', 'cowboys', 'and', 'homoeroticism', 'in', 'cinema', 'instead', 'of', 'anything', 'useful', 'for', 'my', 'thesis']}\n",
      "{'id': '1170285336221638656_twitter', 'annotators': {'label': [1, 1, 1], 'annotator_id': [8, 6, 115], 'target': [['None'], ['None'], ['None']]}, 'rationales': [], 'post_tokens': ['y', 'si', 'fuera', 'top', '<number>', 'me', 'faltar√≠an', 'how', 'to', 'get', 'away', 'with', 'murder', 'gossip', 'girl', 'the', 'last', 'ship', 'orphan', 'black', 'downton', 'abbey']}\n",
      "{'id': '1179099934731190272_twitter', 'annotators': {'label': [1, 1, 1], 'annotator_id': [17, 1, 18], 'target': [['None'], ['None'], ['None']]}, 'rationales': [], 'post_tokens': ['<user>', '<user>', '<user>', 'i', 'am', 'a', 'lesbian', 'not', 'queer', 'and', 'quite', 'frankly', 'his', 'disco', 'tits', 'video', 'was', 'repulsive', 'if', 'he', 'thinks', 'being', 'an', 'overweight', 'gay', 'bloke', 'brings', 'out', 'the', 'woman', 'in', 'him', 'then', 'i', 'am', 'offended', 'it', 'pathetic', 'attention', 'seeking']}\n",
      "{'id': '1178856372617846789_twitter', 'annotators': {'label': [1, 1, 1], 'annotator_id': [17, 9, 1], 'target': [['None'], ['None'], ['None']]}, 'rationales': [], 'post_tokens': ['<user>', 'by', 'tweeting', 'about', 'a', 'civil', 'war', 'trump', 'is', 'sending', 'a', 'message', 'to', 'all', 'his', 'gun', 'toting', 'proud', 'boy', 'nazi', 'white', 'supremacist', 'right', 'wing', 'nutjobs', 'to', 'crawl', 'out', 'of', 'the', 'shadows', 'rebel', 'desperate', 'trump', 'will', 'do', 'dangerous', 'things']}\n",
      "{'id': '1178878849570021376_twitter', 'annotators': {'label': [1, 1, 1], 'annotator_id': [25, 39, 17], 'target': [['None'], ['None'], ['None']]}, 'rationales': [], 'post_tokens': ['<user>', '<user>', 'you', 'all', 'only', 'caring', 'about', 'their', 'hair', 'if', 'these', 'kids', 'were', 'white', 'it', 'would', 'be', 'different', 'latinos', 'love', 'playing', 'both', 'sides', 'they', 'your', 'homeboy', 'until', 'u', 'say', 'mi', 'amigo', 'or', 'deportation', 'then', 'u', 'a', 'porch', 'monkey', 'i', 'could', 'never']}\n",
      "{'id': '1262779554093039616_twitter', 'annotators': {'label': [1, 1, 1], 'annotator_id': [204, 215, 221], 'target': [['Women', 'Other'], ['None'], ['Women']]}, 'rationales': [], 'post_tokens': ['<user>', 'number', 'of', 'people', 'being', 'born', 'under', 'the', 'poverty', 'line', 'all', 'time', 'high', 'too', 'but', 'i', 'guess', 'let', 'push', 'for', 'some', 'retarded', 'pseudo', 'feminism']}\n",
      "{'id': '1179096698498568192_twitter', 'annotators': {'label': [1, 1, 1], 'annotator_id': [9, 17, 4], 'target': [['None'], ['None'], ['Women']]}, 'rationales': [], 'post_tokens': ['you', 'bitches', 'are', 'retarded', 'learn', 'your', 'self', 'worth', 'stop', 'trying', 'to', 'come', 'for', 'the', 'female', 'and', 'correct', 'your', 'wack', 'ass', 'man']}\n",
      "{'id': '1092101046820687872_twitter', 'annotators': {'label': [1, 1, 1], 'annotator_id': [204, 207, 221], 'target': [['None'], ['None'], ['None']]}, 'rationales': [], 'post_tokens': ['<user>', 'argentina', 'still', 'has', 'a', 'big', 'nazi', 'following', 'in', 'some', 'parts', 'the', 'ideology', 'is', 'alive', 'and', 'well', 'there', 'to', 'this', 'day']}\n",
      "{'id': '1178924924313882624_twitter', 'annotators': {'label': [1, 1, 1], 'annotator_id': [4, 84, 1], 'target': [['Homosexual'], ['None'], ['None']]}, 'rationales': [], 'post_tokens': ['queer', 'eye', 'but', 'it', 'straight', 'people', 'making', 'over', 'gays', 'and', 'it', 'conversion', 'therapy']}\n",
      "{'id': '1178222483314479107_twitter', 'annotators': {'label': [1, 1, 1], 'annotator_id': [54, 9, 10], 'target': [['None'], ['None'], ['None']]}, 'rationales': [], 'post_tokens': ['<user>', '<user>', '<user>', 'i', 'love', 'you']}\n",
      "983\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i, row in enumerate(val_data):\n",
    "    labels = row['annotators']['label']\n",
    "    if len(set(labels))==1:\n",
    "        count+=1\n",
    "    if i <10:\n",
    "        print(row)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: 15383}\n",
      "count_mistmatch 0\n"
     ]
    }
   ],
   "source": [
    "#Focus on train data for now\n",
    "#Observe the train data\n",
    "\n",
    "#Count the number of annotators\n",
    "count_annotators ={}\n",
    "count_mistmatch = 0\n",
    "for data in train_data:\n",
    "    count = len(data['annotators']['annotator_id'])\n",
    "    if count !=len(data[\"annotators\"][\"label\"]) or count !=count != len(data[\"annotators\"][\"target\"]):\n",
    "        count_mistmatch +=1\n",
    "    if count in count_annotators:\n",
    "        count_annotators[count] +=1\n",
    "    else:\n",
    "        count_annotators[count] = 1\n",
    "\n",
    "print(count_annotators)\n",
    "print(\"count_mistmatch\", count_mistmatch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_rationale_match_annotators:  3518\n",
      "count_rational_not_match_annotators 11865\n",
      "{2: 5614, 3: 3518, 0: 6251}\n"
     ]
    }
   ],
   "source": [
    "#count an item where the number of annotators don't match the number of rationales\n",
    "count_rationale_match_annotators = 0\n",
    "count_rational_not_match_annotators = 0\n",
    "count_rationales = {}\n",
    "for data in train_data:\n",
    "    count = len(data['rationales'])\n",
    "    if count in count_rationales:\n",
    "        count_rationales[count] += 1\n",
    "    else:\n",
    "        count_rationales[count] = 1\n",
    "        \n",
    "    if len(data['annotators'][\"label\"]) == count:\n",
    "        count_rationale_match_annotators += 1\n",
    "    else:\n",
    "        count_rational_not_match_annotators += 1\n",
    "        \n",
    "print(\"count_rationale_match_annotators: \", count_rationale_match_annotators)\n",
    "print(\"count_rational_not_match_annotators\", count_rational_not_match_annotators)\n",
    "print(count_rationales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_rationale_normal (all three anotators thought the speech was normal) 4096\n",
      "no_rationale_else (not all through it was normal) 2155\n",
      "no_rationale_abnormal (the majority vote was not normal):  0\n"
     ]
    }
   ],
   "source": [
    "#Are there data labeled as offensive or hate speech but no annotation?\n",
    "no_rationale_data_list = []\n",
    "for data in train_data:\n",
    "    count = len(data[\"rationales\"])\n",
    "    if count == 0:\n",
    "        no_rationale_data_list.append(data[\"annotators\"][\"label\"])\n",
    "\n",
    "no_rationale_normal = []\n",
    "no_rationale_else = []\n",
    "for data in no_rationale_data_list:\n",
    "    if data[0] == data[1] == data[2] ==1: #normal\n",
    "        no_rationale_normal.append(data)\n",
    "    else:\n",
    "        no_rationale_else.append(data)\n",
    "\n",
    "print(\"no_rationale_normal (all three anotators thought the speech was normal)\", len(no_rationale_normal))\n",
    "print(\"no_rationale_else (not all through it was normal)\", len(no_rationale_else))\n",
    "\n",
    "no_rationale_abnormal = []\n",
    "for data in no_rationale_else:\n",
    "    count = 0\n",
    "    for i in data:\n",
    "        if i == 1:\n",
    "            count +=1\n",
    "    if count < 2:\n",
    "        no_rationale_abnormal.append(data)\n",
    "print(\"no_rationale_abnormal (the majority vote was not normal): \", len(no_rationale_abnormal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_hatel_label_match_rationales:  9016\n",
      "count_hatel_label_no_match_rationales 6367\n"
     ]
    }
   ],
   "source": [
    "#check # of rationales vs # of # of hate speech labels\n",
    "count_hate_label_match_rationales =0\n",
    "count_hate_label_no_match_rationales =0\n",
    "for data in train_data:\n",
    "    count_hate_label = data[\"annotators\"][\"label\"].count(0)\n",
    "    count_rationales = len(data[\"rationales\"])\n",
    "    if count_hate_label == count_rationales:\n",
    "        count_hate_label_match_rationales += 1\n",
    "    else:\n",
    "        count_hate_label_no_match_rationales += 1\n",
    "        \n",
    "print(\"count_hate_label_match_rationales: \", count_hate_label_match_rationales)\n",
    "print(\"count_hate_label_no_match_rationales\", count_hate_label_no_match_rationales)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['Refugee'], ['Refugee'], ['None']], [['Arab', 'Caucasian'], ['None'], ['Arab']], [['Women'], ['None'], ['Women']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['Islam', 'Women'], ['Arab', 'Islam', 'Women'], ['Arab', 'Islam']], [['None'], ['African'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['Other'], ['None'], ['Other']], [['None'], ['Women', 'Homosexual'], ['None']], [['None'], ['Other'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['Other'], ['None']], [['None'], ['None'], ['None']], [['None'], ['Women', 'None'], ['Other']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['Islam'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['Caucasian'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['Women'], ['Women'], ['Women']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['Caucasian'], ['None']], [['Other', 'Caucasian'], ['None'], ['None']], [['None'], ['African', 'Asian', 'Caucasian', 'Hispanic'], ['Caucasian']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['Men', 'Women']], [['None'], ['None'], ['None']], [['Other'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['Women'], ['Other'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['Homosexual'], ['None'], ['None']], [['None'], ['None'], ['None']], [['Homosexual', 'Other'], ['Homosexual'], ['Homosexual']], [['Jewish'], ['None'], ['None']], [['None'], ['None'], ['Refugee']], [['None'], ['None'], ['None']], [['Men', 'African'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['Homosexual'], ['Homosexual'], ['None']], [['None'], ['None'], ['None']], [['Homosexual'], ['Homosexual'], ['None']], [['None'], ['None'], ['None']], [['Jewish'], ['Jewish'], ['Jewish']], [['Islam'], ['None'], ['Islam']], [['Women'], ['Women'], ['None']], [['Women'], ['None'], ['Women']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['Caucasian'], ['None']], [['None'], ['None'], ['None']], [['None'], ['Caucasian'], ['None']], [['None'], ['None'], ['None']], [['Women'], ['Women'], ['Women']], [['None'], ['None'], ['None']], [['None'], ['Women'], ['Women']], [['Refugee'], ['Refugee'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['Homosexual'], ['None']], [['None'], ['None'], ['None']], [['Men', 'Women'], ['None'], ['Refugee']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['Refugee'], ['Refugee'], ['Refugee']], [['None'], ['None'], ['None']], [['None'], ['None'], ['None']], [['None'], ['None'], ['Refugee', 'Other']], [['Other'], ['None'], ['Refugee']], [['None'], ['None'], ['None']]]\n"
     ]
    }
   ],
   "source": [
    "#Check target\n",
    "target_if_normal = []\n",
    "for data in train_data:\n",
    "    labels = data[\"annotators\"][\"label\"]\n",
    "    if labels[0]==labels[1]==labels[2] == 1: #normal\n",
    "        target_if_normal.append(data[\"annotators\"][\"target\"])\n",
    "\n",
    "print(target_if_normal[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'absent': {'hate': 0, 'normal': 6251, 'offensive': 0}, 'present': {2: {'hate': 2049, 'normal': 0, 'offensive': 3565}, 3: {'hate': 2699, 'offensive': 819}}, 'match': {2: {'hate': 383, 'offensive': 866}, 3: {'hate': 693, 'offensive': 282}}, 'no_match': {2: {'hate': 1666, 'offensive': 2699}, 3: {'hate': 2006, 'offensive': 537}}}\n",
      "[{'id': '23107796_gab', 'annotators': {'label': [0, 2, 2], 'annotator_id': [203, 204, 233], 'target': [['Hindu', 'Islam'], ['Hindu', 'Islam'], ['Hindu', 'Islam', 'Other']]}, 'rationales': [[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'post_tokens': ['u', 'really', 'think', 'i', 'would', 'not', 'have', 'been', 'raped', 'by', 'feral', 'hindu', 'or', 'muslim', 'back', 'in', 'india', 'or', 'bangladesh', 'and', 'a', 'neo', 'nazi', 'would', 'rape', 'me', 'as', 'well', 'just', 'to', 'see', 'me', 'cry']}, {'id': '1227920812235051008_twitter', 'annotators': {'label': [2, 2, 2], 'annotator_id': [209, 203, 222], 'target': [['Other'], ['Hindu'], ['Hindu', 'Other']]}, 'rationales': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]], 'post_tokens': ['if', 'english', 'is', 'not', 'imposition', 'then', 'hindi', 'is', 'also', 'not', 'imposition', 'shut', 'up', 'chutiya', 'retards', 'stophindiimposition']}, {'id': '1143683682869436416_twitter', 'annotators': {'label': [1, 0, 0], 'annotator_id': [217, 206, 223], 'target': [['Men', 'Women'], ['Refugee'], ['Refugee']]}, 'rationales': [[0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]], 'post_tokens': ['<user>', '<number>', 'americans', 'dead', 'because', 'of', 'an', 'illegal', 'immigrant', 'this', 'is', 'infuriating', 'enforce', 'our', 'laws', 'deport', 'them']}, {'id': '17847556_gab', 'annotators': {'label': [2, 0, 0], 'annotator_id': [200, 223, 231], 'target': [['Refugee'], ['Other'], ['Refugee']]}, 'rationales': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'post_tokens': ['thank', 'a', 'white', 'man', 'you', 'live', 'in', 'his', 'world', 'and', 'why', 'do', 'you', 'islanders', 'say', 'you', 'are', 'canadian', 'so', 'rude', 'really', 'if', 'i', 'lived', 'in', 'a', 'brown', 'country', 'even', 'with', 'citizenship', 'i', 'would', 'never', 'call', 'myself', 'ethiopian', 'chinese', 'indian', 'you', 'are', 'a', 'caribbean', 'living', 'in', 'canada']}, {'id': '21760024_gab', 'annotators': {'label': [0, 0, 1], 'annotator_id': [213, 210, 228], 'target': [['Asian', 'Other'], ['Asian', 'Christian'], ['None']]}, 'rationales': [[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]], 'post_tokens': ['that', 'a', 'retarded', 'non', 'argument', 'the', 'bible', 'is', 'also', 'a', 'best', 'seller', 'in', 'communist', 'china']}]\n"
     ]
    }
   ],
   "source": [
    "#check annotations\n",
    "from collections import Counter\n",
    "annotation_count = {\"absent\": {\"hate\": 0, \"normal\": 0, \"offensive\":0}, \"present\": {2: {\"hate\": 0, \"normal\": 0, \"offensive\":0}, 3: {\"hate\": 0, \"offensive\":0}}, \"match\": {2: {\"hate\": 0,  \"offensive\":0}, 3: {\"hate\": 0, \"offensive\":0}}, \"no_match\": {2: {\"hate\": 0, \"offensive\":0}, 3: {\"hate\": 0, \"offensive\":0}}}\n",
    "no_match_examples = []\n",
    "def get_majority_element(lst):\n",
    "        # Count occurrences of each element\n",
    "        count = Counter(lst)\n",
    "        \n",
    "        # Check if there is a majority (i.e., an element that appears at least twice)\n",
    "        if count.most_common(1)[0][1] >= 2:\n",
    "            return count.most_common(1)[0][0]\n",
    "        else:\n",
    "            return 3  # Indicate undecided if no majority\n",
    "        \n",
    "for data in train_data:\n",
    "    annotations = data[\"rationales\"]\n",
    "    labels = data[\"annotators\"]['label']\n",
    "    label_token = get_majority_element(labels)\n",
    "    label_mapping={0: \"hate\", 1: \"normal\", 2: \"offensive\", 3: \"undecided\"}\n",
    "    label = label_mapping[label_token]\n",
    "    if len(annotations)==0:\n",
    "        annotation_count[\"absent\"][label]+=1\n",
    "    else:\n",
    "        annotation_count[\"present\"][len(annotations)][label]+=1\n",
    "        if len(annotations)==2:\n",
    "            if annotations[0]==annotations[1]:\n",
    "                annotation_count[\"match\"][len(annotations)][label]+=1\n",
    "            else:\n",
    "                annotation_count[\"no_match\"][len(annotations)][label]+=1\n",
    "                no_match_examples.append(data)\n",
    "        elif len(annotations)==3:\n",
    "            if annotations[0]==annotations[1] and annotations[1]==annotations[2]:\n",
    "                annotation_count[\"match\"][len(annotations)][label]+=1\n",
    "            else:\n",
    "                annotation_count[\"no_match\"][len(annotations)][label]+=1\n",
    "print(annotation_count)\n",
    "print(no_match_examples[:5])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u really think i would not have been raped by feral hindu or muslim back in india or bangladesh and a neo nazi would rape me as well just to see me cry\n",
      "rationale: \n",
      "raped\n",
      "neo\n",
      "nazi\n",
      "would\n",
      "rape\n",
      "\n",
      "rationale: \n",
      "raped\n",
      "feral\n",
      "hindu\n",
      "muslim\n",
      "neo\n",
      "nazi\n",
      "\n",
      "if english is not imposition then hindi is also not imposition shut up chutiya retards stophindiimposition\n",
      "rationale: \n",
      "chutiya\n",
      "retards\n",
      "stophindiimposition\n",
      "\n",
      "rationale: \n",
      "retards\n",
      "\n",
      "<user> <number> americans dead because of an illegal immigrant this is infuriating enforce our laws deport them\n",
      "rationale: \n",
      "<number>\n",
      "americans\n",
      "dead\n",
      "because\n",
      "of\n",
      "an\n",
      "illegal\n",
      "immigrant\n",
      "\n",
      "rationale: \n",
      "deport\n",
      "them\n",
      "\n",
      "thank a white man you live in his world and why do you islanders say you are canadian so rude really if i lived in a brown country even with citizenship i would never call myself ethiopian chinese indian you are a caribbean living in canada\n",
      "rationale: \n",
      "brown\n",
      "country\n",
      "you\n",
      "are\n",
      "a\n",
      "caribbean\n",
      "living\n",
      "in\n",
      "canada\n",
      "\n",
      "rationale: \n",
      "and\n",
      "why\n",
      "do\n",
      "you\n",
      "islanders\n",
      "say\n",
      "you\n",
      "are\n",
      "canadian\n",
      "so\n",
      "rude\n",
      "really\n",
      "if\n",
      "i\n",
      "lived\n",
      "in\n",
      "a\n",
      "brown\n",
      "country\n",
      "even\n",
      "with\n",
      "citizenship\n",
      "i\n",
      "would\n",
      "never\n",
      "call\n",
      "myself\n",
      "ethiopian\n",
      "chinese\n",
      "indian\n",
      "you\n",
      "are\n",
      "a\n",
      "caribbean\n",
      "living\n",
      "in\n",
      "canada\n",
      "\n",
      "that a retarded non argument the bible is also a best seller in communist china\n",
      "rationale: \n",
      "retarded\n",
      "\n",
      "rationale: \n",
      "retarded\n",
      "communist\n",
      "china\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#visualize annotations\n",
    "def visualize(data):\n",
    "    post_tokens = data[\"post_tokens\"]\n",
    "    rationales = data[\"rationales\"]\n",
    "    \n",
    "    sentence = \" \".join(post_tokens)  # Joins words with a space in between\n",
    "    print(sentence)\n",
    "\n",
    "    for rationale in rationales:\n",
    "        print(\"rationale: \")\n",
    "        for i in range(len(rationale)):\n",
    "            if rationale[i]==1:\n",
    "                print(post_tokens[i])\n",
    "        print()\n",
    "for j in range(5):\n",
    "    visualize(no_match_examples[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'match': 9131, 'no_match': 1}\n",
      "[{'id': '24439295_gab', 'annotators': {'label': [2, 2, 0], 'annotator_id': [222, 209, 201], 'target': [['Homosexual'], ['Other'], ['Homosexual']]}, 'rationales': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1]], 'post_tokens': ['my', 'rhymes', 'pass', 'any', 'bar', 'exam', 'they', 'call', 'me', 'kincannon', '<number>', 'shots', 'to', 'your', 'think', 'tank', 'im', 'wavin', 'twin', 'cannons', 'you', 'a', 'fag', 'that', 'loves', 'it', 'up', 'the', 'ass', 'you', 'are', 'a', 'ken', 'kaniff', 'and', 'yo', 'bitch', 'gimme', 'sloppy', 'toppy', 'they', 'call', 'her', 'steve', 'bannon', 'the', 'g.o.a.t.']}]\n",
      "47\n",
      "47\n",
      "48\n",
      "my rhymes pass any bar exam they call me kincannon <number> shots to your think tank im wavin twin cannons you a fag that loves it up the ass you are a ken kaniff and yo bitch gimme sloppy toppy they call her steve bannon the g.o.a.t.\n",
      "rationale: \n",
      "you\n",
      "a\n",
      "fag\n",
      "that\n",
      "loves\n",
      "it\n",
      "up\n",
      "the\n",
      "ass\n",
      "you\n",
      "are\n",
      "a\n",
      "ken\n",
      "kaniff\n",
      "and\n",
      "yo\n",
      "bitch\n",
      "gimme\n",
      "sloppy\n",
      "toppy\n",
      "they\n",
      "call\n",
      "her\n",
      "steve\n",
      "bannon\n",
      "the\n",
      "g.o.a.t.\n",
      "\n",
      "rationale: \n",
      "fag\n",
      "up\n",
      "the\n",
      "ass\n",
      "bitch\n",
      "sloppy\n",
      "toppy\n",
      "the\n",
      "g.o.a.t.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(no_length_match_samples[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrationales\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(no_length_match_samples[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrationales\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m---> 29\u001b[0m \u001b[43mvisualize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mno_length_match_samples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 13\u001b[0m, in \u001b[0;36mvisualize\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rationale)):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rationale[i]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 13\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[43mpost_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "rationale_length = {\"match\": 0, \"no_match\": 0}\n",
    "no_length_match_samples = []\n",
    "\n",
    "for data in train_data:\n",
    "    rationales = data['rationales']\n",
    "    if len(rationales)==0:\n",
    "        continue\n",
    "    sentence_length_1 = len(rationales[0])\n",
    "    sentence_length_2 = len(rationales[1])\n",
    "    if sentence_length_1 !=sentence_length_2:\n",
    "        rationale_length[\"no_match\"] +=1\n",
    "        no_length_match_samples.append(data)\n",
    "    if len(rationales)==2:\n",
    "        if sentence_length_1 ==sentence_length_2:\n",
    "            rationale_length[\"match\"] +=1\n",
    "        continue\n",
    "    if sentence_length_2 !=len(rationales[2]):\n",
    "        rationale_length[\"no_match\"] +=1\n",
    "        no_length_match_samples.append(data)\n",
    "    else:\n",
    "        rationale_length[\"match\"] +=1\n",
    "print(rationale_length)\n",
    "print(no_length_match_samples)\n",
    "#There is one sample where length does not match\n",
    "\n",
    "print(len(no_length_match_samples[0][\"post_tokens\"]))\n",
    "print(len(no_length_match_samples[0][\"rationales\"][0]))\n",
    "print(len(no_length_match_samples[0][\"rationales\"][1])) #Cut the last word since it's one word off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '1190398852601384960_twitter', 'annotators': {'label': [1, 2, 2], 'annotator_id': [236, 212, 211], 'target': [['Asian'], ['Asian'], ['Asian']]}, 'rationales': [[0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0], [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]], 'post_tokens': ['bro', 'why', 'do', 'asians', 'have', 'a', 'low', 'alcohol', 'tolerance', 'this', 'is', 'not', 'fair']}\n"
     ]
    }
   ],
   "source": [
    "for data in train_data: \n",
    "    if data[\"id\"] == \"1190398852601384960_twitter\":\n",
    "        print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4008588_gab\n",
      "[1.0, 0, 0, 1.0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 1.0, 1.0, 0, 0, 0]\n",
      "4004699_gab\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 1.0, 1.0, 1.0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "for data in train_data:\n",
    "    post_tokens = data[\"post_tokens\"]\n",
    "    if post_tokens == ['muslims', 'have', 'been', 'enslaving', 'and', 'raping', 'white', 'women', 'for', '<number>', 'years', 'european', 'painters', 'were', 'calling', 'attention', 'to', 'this', 'for', 'centuries', 'today', 'no', 'one', 'cares', 'enjoy', 'being', 'raped', 'european', 'white', 'women']:\n",
    "        print(data[\"id\"])\n",
    "        keywords = [\"muslims\", \"enslaving\", \"raping\", \"enjoy\", \"being\", \"raped\" ]\n",
    "        rationale = [0 for post in range( len(post_tokens))]\n",
    "        for i in range(len(post_tokens)):\n",
    "            if post_tokens[i] in keywords:\n",
    "                rationale[i] = 1.0\n",
    "        print (rationale)\n",
    "    if post_tokens == ['<user>', 'not', 'good', 'good', 'vs', 'bad', 'more', 'like', 'they', 'both', 'suck', 'choosing', 'sides', 'is', 'what', 'the', 'zionist', 'and', 'the', 'arabs', 'like', 'i', 'could', 'care', 'less', 'if', 'they', 'kill', 'each', 'other', 'nobloodforisrael', 'we', 'have', 'our', 'own', 'problems', 'our', 'gov', 'is', 'commitiing', 'high', 'treason', 'while', 'being', 'tyrannical']:\n",
    "        print(data[\"id\"])\n",
    "        keywords = [\"zionist\", \"arabs\", \"kill\", \"each\", \"other\", \"nobloodforisrael\" ]\n",
    "        rationale = [0 for post in range( len(post_tokens))]\n",
    "        for i in range(len(post_tokens)):\n",
    "            if post_tokens[i] in keywords:\n",
    "                rationale[i] = 1.0\n",
    "        print (rationale)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's process the training/eval data\n",
    "from collections import Counter\n",
    "\n",
    "def compute_average_attention(rationales):\n",
    "    \"\"\"\n",
    "    Computes the average attention value for each token across a list of rationales.\n",
    "\n",
    "    Args:\n",
    "        rationales (list of lists): Each rationale is a list representing token-level attention (0 or 1).\n",
    "\n",
    "    Returns:\n",
    "        list: Average attention vector for each token.\n",
    "    \"\"\"\n",
    "    # Number of rationales\n",
    "    num_rationales = len(rationales)\n",
    "\n",
    "    # Sentence length (assuming all rationales are the same length)\n",
    "    sentence_length = len(rationales[0])\n",
    "\n",
    "    # Initialize a list to store the sums of token-wise values\n",
    "    token_sums = [0] * sentence_length\n",
    "\n",
    "    # Sum up the token-wise values for each rationale\n",
    "    for rationale in rationales:\n",
    "        for i, value in enumerate(rationale):\n",
    "            token_sums[i] += value\n",
    "\n",
    "    # Compute the average by dividing the sums by the number of rationales\n",
    "    average_attention = [token_sum / num_rationales for token_sum in token_sums]\n",
    "\n",
    "    return average_attention\n",
    "\n",
    "def process_easy_data(dataset):\n",
    "    def get_majority_element(lst):\n",
    "        # Count occurrences of each element\n",
    "        count = Counter(lst)\n",
    "        \n",
    "        # Check if there is a majority (i.e., an element that appears at least twice)\n",
    "        if count.most_common(1)[0][1] >= 2:\n",
    "            return count.most_common(1)[0][0]\n",
    "        else:\n",
    "            return 3  # Indicate undecided if no majority\n",
    "        \n",
    "    easy_data =[]\n",
    "    for data in dataset:\n",
    "        labels = data[\"annotators\"]['label']\n",
    "        targets = data[\"annotators\"][\"target\"]\n",
    "        rationales = data[\"rationales\"]\n",
    "        id = data[\"id\"]\n",
    "        \n",
    "        final_label = get_majority_element(labels)\n",
    "        \n",
    "        final_targets = []\n",
    "        target_dicts = {}\n",
    "        for target in targets:\n",
    "            for word in target:\n",
    "                if word in target_dicts:\n",
    "                    target_dicts[word] += 1\n",
    "                else:\n",
    "                    target_dicts[word] = 1\n",
    "        for dict in target_dicts:\n",
    "            if target_dicts[dict] >=2:\n",
    "                final_targets.append(dict)\n",
    "                \n",
    "        #fix the bug in the dataset\n",
    "        if id ==\"24439295_gab\":\n",
    "            rationales[1].pop() #fix the length mismatch\n",
    "            \n",
    "        if len(rationales)==0:\n",
    "            rationale = []\n",
    "        else:\n",
    "            #fix the bug\n",
    "            if id == \"4008588_gab\":\n",
    "                rationale = [1.0, 0, 0, 1.0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 1.0, 1.0, 0, 0, 0]\n",
    "            elif id == \"4004699_gab\":\n",
    "                rationale = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 1.0, 1.0, 1.0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]           \n",
    "            else:\n",
    "                rationale = compute_average_attention(rationales)\n",
    "                                              \n",
    "        final_data = {}\n",
    "        final_data[\"id\"] = id \n",
    "        final_data[\"post_tokens\"] = data[\"post_tokens\"]\n",
    "        final_data[\"label\"] = final_label\n",
    "        final_data[\"target\"] = final_targets\n",
    "        final_data[\"rationale\"] = rationale\n",
    "        easy_data.append(final_data)\n",
    "    return easy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "easy_train_data length:  15383\n",
      "[{'id': '23107796_gab', 'post_tokens': ['u', 'really', 'think', 'i', 'would', 'not', 'have', 'been', 'raped', 'by', 'feral', 'hindu', 'or', 'muslim', 'back', 'in', 'india', 'or', 'bangladesh', 'and', 'a', 'neo', 'nazi', 'would', 'rape', 'me', 'as', 'well', 'just', 'to', 'see', 'me', 'cry'], 'label': 2, 'target': ['Hindu', 'Islam'], 'rationale': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.5, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, {'id': '9995600_gab', 'post_tokens': ['the', 'uk', 'has', 'threatened', 'to', 'return', 'radioactive', 'waste', 'to', 'the', 'eu', 'if', 'an', 'agreement', 'cannot', 'be', 'reached', 'can', 'not', 'we', 'keep', 'the', 'radioactive', 'waste', 'and', 'send', 'back', 'all', 'the', 'paki', 'migrants', 'instead'], 'label': 2, 'target': ['Refugee', 'Indian'], 'rationale': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 1.0, 0.6666666666666666, 0.3333333333333333]}, {'id': '1227920812235051008_twitter', 'post_tokens': ['if', 'english', 'is', 'not', 'imposition', 'then', 'hindi', 'is', 'also', 'not', 'imposition', 'shut', 'up', 'chutiya', 'retards', 'stophindiimposition'], 'label': 2, 'target': ['Other', 'Hindu'], 'rationale': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.5]}, {'id': '1204931715778543624_twitter', 'post_tokens': ['no', 'liberal', 'congratulated', 'hindu', 'refugees', 'post', 'cab', 'because', 'they', 'hate', 'hindus'], 'label': 2, 'target': ['Hindu'], 'rationale': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]}, {'id': '1179102559241244672_twitter', 'post_tokens': ['he', 'said', 'bro', 'even', 'your', 'texts', 'sound', 'redneck', 'whatever', 'the', 'fuck', 'that', 'means', 'üòÇ', 'ü§∑', 'üèª', '\\u200d', '‚ôÇÔ∏è', '<user>'], 'label': 2, 'target': ['Economic'], 'rationale': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, {'id': '18790322_gab', 'post_tokens': ['i', 'live', 'and', 'work', 'with', 'many', 'legal', 'mexican', 'immigrants', 'who', 'are', 'great', 'citizens', 'and', 'trump', 'supporters', 'they', 'have', 'no', 'problem', 'with', 'deporting', 'illegals', 'maga'], 'label': 1, 'target': ['Hispanic', 'Refugee'], 'rationale': []}, {'id': '13459307_gab', 'post_tokens': ['in', 'britain', 'the', 'prime', 'minister', 'a', 'childless', 'woman', 'london', 'mayor', 'a', 'moslem', 'invader', 'in', 'command', 'of', 'london', 'police', 'a', 'lesbian', 'in', 'command', 'of', 'london', 'fire', 'brigade', 'a', 'fat', 'woman', 'scotland', 'first', 'minister', 'a', 'childless', 'woman'], 'label': 0, 'target': ['Women', 'Disability', 'Islam', 'Homosexual'], 'rationale': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.0, 0.3333333333333333, 0.3333333333333333]}, {'id': '1113705567782969344_twitter', 'post_tokens': ['<user>', '<user>', 'there', 'shd', 'be', 'a', 'film', 'called', 'no', 'hindus', 'in', 'kashmir', 'wud', 'be', 'perfect', 'islamic', 'fanaticsm', 'movement', 'in', 'kashmir'], 'label': 1, 'target': ['Hindu', 'Islam'], 'rationale': []}, {'id': '24555628_gab', 'post_tokens': ['jews', 'defends', 'jews', 'regardless', 'muslims', 'defend', 'muslins', 'regardless', 'christian', 'european', 'attack', 'christians', 'christian', 'people', 'are', 'on', 'their', 'own', 'and', 'jews', 'and', 'muslims', 'have', 'another', 'home', 'we', 'don', 'ffs', 'we', 'do', 'not', 'why', 'can', 'not', 'people', 'understand', 'that', 'it', 'not', 'that', 'difficult'], 'label': 1, 'target': ['Christian', 'Islam'], 'rationale': []}, {'id': '16382456_gab', 'post_tokens': ['the', 'non', 'partisan', 'congressional', 'budget', 'office', 'cbo', 'has', 'released', 'their', 'report', 'on', 'sens', 'lindsey', 'graham', 'and', 'dick', 'durbin', '<number>', 'the', 'dream', 'act', 'the', 'report', 'shows', 'that', 'their', 'legislation', 'would', 'add', '<number>', 'billion', 'to', 'the', 'deficit', 'over', 'a', 'ten', 'year', 'period', 'and', 'lots', 'more', 'immigrant', 'scum'], 'label': 0, 'target': ['Refugee'], 'rationale': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]}]\n"
     ]
    }
   ],
   "source": [
    "#Processing train data\n",
    "import numpy as np\n",
    "easy_train_data = process_easy_data(train_data)\n",
    "print(\"easy_train_data length: \", len(easy_train_data))\n",
    "print(easy_train_data[:10])\n",
    "# Save list as a JSON file\n",
    "with open('easy_train_data.json', 'w') as f:\n",
    "    json.dump(easy_train_data, f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "easy_eval_data length:  1922\n",
      "[{'id': '1178613994371928065_twitter', 'post_tokens': ['me', 'getting', 'books', 'from', 'the', 'library', 'about', 'queer', 'cowboys', 'and', 'homoeroticism', 'in', 'cinema', 'instead', 'of', 'anything', 'useful', 'for', 'my', 'thesis'], 'label': 1, 'target': ['None'], 'rationale': []}, {'id': '1170285336221638656_twitter', 'post_tokens': ['y', 'si', 'fuera', 'top', '<number>', 'me', 'faltar√≠an', 'how', 'to', 'get', 'away', 'with', 'murder', 'gossip', 'girl', 'the', 'last', 'ship', 'orphan', 'black', 'downton', 'abbey'], 'label': 1, 'target': ['None'], 'rationale': []}, {'id': '1179099934731190272_twitter', 'post_tokens': ['<user>', '<user>', '<user>', 'i', 'am', 'a', 'lesbian', 'not', 'queer', 'and', 'quite', 'frankly', 'his', 'disco', 'tits', 'video', 'was', 'repulsive', 'if', 'he', 'thinks', 'being', 'an', 'overweight', 'gay', 'bloke', 'brings', 'out', 'the', 'woman', 'in', 'him', 'then', 'i', 'am', 'offended', 'it', 'pathetic', 'attention', 'seeking'], 'label': 1, 'target': ['None'], 'rationale': []}, {'id': '1178856372617846789_twitter', 'post_tokens': ['<user>', 'by', 'tweeting', 'about', 'a', 'civil', 'war', 'trump', 'is', 'sending', 'a', 'message', 'to', 'all', 'his', 'gun', 'toting', 'proud', 'boy', 'nazi', 'white', 'supremacist', 'right', 'wing', 'nutjobs', 'to', 'crawl', 'out', 'of', 'the', 'shadows', 'rebel', 'desperate', 'trump', 'will', 'do', 'dangerous', 'things'], 'label': 1, 'target': ['None'], 'rationale': []}, {'id': '1178878849570021376_twitter', 'post_tokens': ['<user>', '<user>', 'you', 'all', 'only', 'caring', 'about', 'their', 'hair', 'if', 'these', 'kids', 'were', 'white', 'it', 'would', 'be', 'different', 'latinos', 'love', 'playing', 'both', 'sides', 'they', 'your', 'homeboy', 'until', 'u', 'say', 'mi', 'amigo', 'or', 'deportation', 'then', 'u', 'a', 'porch', 'monkey', 'i', 'could', 'never'], 'label': 1, 'target': ['None'], 'rationale': []}, {'id': '1262779554093039616_twitter', 'post_tokens': ['<user>', 'number', 'of', 'people', 'being', 'born', 'under', 'the', 'poverty', 'line', 'all', 'time', 'high', 'too', 'but', 'i', 'guess', 'let', 'push', 'for', 'some', 'retarded', 'pseudo', 'feminism'], 'label': 1, 'target': ['Women'], 'rationale': []}, {'id': '1179096698498568192_twitter', 'post_tokens': ['you', 'bitches', 'are', 'retarded', 'learn', 'your', 'self', 'worth', 'stop', 'trying', 'to', 'come', 'for', 'the', 'female', 'and', 'correct', 'your', 'wack', 'ass', 'man'], 'label': 1, 'target': ['None'], 'rationale': []}, {'id': '1092101046820687872_twitter', 'post_tokens': ['<user>', 'argentina', 'still', 'has', 'a', 'big', 'nazi', 'following', 'in', 'some', 'parts', 'the', 'ideology', 'is', 'alive', 'and', 'well', 'there', 'to', 'this', 'day'], 'label': 1, 'target': ['None'], 'rationale': []}, {'id': '1178924924313882624_twitter', 'post_tokens': ['queer', 'eye', 'but', 'it', 'straight', 'people', 'making', 'over', 'gays', 'and', 'it', 'conversion', 'therapy'], 'label': 1, 'target': ['None'], 'rationale': []}, {'id': '1178222483314479107_twitter', 'post_tokens': ['<user>', '<user>', '<user>', 'i', 'love', 'you'], 'label': 1, 'target': ['None'], 'rationale': []}]\n"
     ]
    }
   ],
   "source": [
    "#Process eval data\n",
    "easy_eval_data = process_easy_data(val_data)\n",
    "print(\"easy_eval_data length: \", len(easy_eval_data))\n",
    "print(easy_eval_data[:10])\n",
    "\n",
    "# Save list as a JSON file\n",
    "with open('easy_eval_data.json', 'w') as f:\n",
    "    json.dump(easy_eval_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "easy_test_data length:  1924\n"
     ]
    }
   ],
   "source": [
    "#Processing train data\n",
    "import numpy as np\n",
    "easy_test_data = process_easy_data(test_data)\n",
    "print(\"easy_test_data length: \", len(easy_test_data))\n",
    "# Save list as a JSON file\n",
    "with open('easy_test_data.json', 'w') as f:\n",
    "    json.dump(easy_test_data, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: 4384, 1: 6251, 0: 4748}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the dataset from a JSON file\n",
    "with open(\"easy_train_data.json\", \"r\") as f:\n",
    "    easy_train_data = json.load(f)\n",
    "\n",
    "# Print the first element in the loaded dataset\n",
    "label_counts ={}\n",
    "for data in easy_train_data:\n",
    "    label = data[\"label\"]\n",
    "    if label in label_counts:\n",
    "        label_counts[label] +=1\n",
    "    else:\n",
    "        label_counts[label] =1\n",
    "print(label_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 781, 0: 593, 2: 548}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the dataset from a JSON file\n",
    "with open(\"easy_eval_data.json\", \"r\") as f:\n",
    "    easy_eval_data = json.load(f)\n",
    "\n",
    "label_counts ={}\n",
    "for data in easy_eval_data:\n",
    "    label = data[\"label\"]\n",
    "    if label in label_counts:\n",
    "        label_counts[label] +=1\n",
    "    else:\n",
    "        label_counts[label] =1\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 782, 2: 548, 0: 594}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the dataset from a JSON file\n",
    "with open(\"easy_test_data.json\", \"r\") as f:\n",
    "    easy_eval_data = json.load(f)\n",
    "\n",
    "label_counts ={}\n",
    "for data in easy_test_data:\n",
    "    label = data[\"label\"]\n",
    "    if label in label_counts:\n",
    "        label_counts[label] +=1\n",
    "    else:\n",
    "        label_counts[label] =1\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'none': 668, 'women': 151, 'african': 310, 'refugee': 77, 'islam': 204, 'homosexual': 182, 'jewish': 186, 'disability': 4, 'caucasian': 48, 'other': 84, 'hispanic': 35, 'arab': 77, 'asian': 34, 'christian': 3, 'men': 6}\n"
     ]
    }
   ],
   "source": [
    "target_counts ={}\n",
    "for data in easy_eval_data:\n",
    "    targets = data['target']\n",
    "    # print(targets)\n",
    "    for target in targets:\n",
    "        target = target.lower()\n",
    "        if target in target_counts:\n",
    "            target_counts[target] += 1\n",
    "        else:\n",
    "            target_counts[target]=12\n",
    "print(target_counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_env",
   "language": "python",
   "name": "hf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9b45dd23733c0aabe177732bf5654ee6d3cbd535f3a5d569b2cfae914988038f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
